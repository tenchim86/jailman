{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction \u00b6 Welcome to Jailam. An open source effort to create an easy management tool for Freenas Jails. As this project is in its early stages, this wiki will serve the purpose of providing a foundation and a almost (not) up-to-date scripting reference to the underlying structure of jailman.","title":"Introduction"},{"location":"#introduction","text":"Welcome to Jailam. An open source effort to create an easy management tool for Freenas Jails. As this project is in its early stages, this wiki will serve the purpose of providing a foundation and a almost (not) up-to-date scripting reference to the underlying structure of jailman.","title":"Introduction"},{"location":"CODE_OF_CONDUCT/","text":"Code of Conduct \u00b6 Our Pledge \u00b6 In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. Our Standards \u00b6 Examples of behavior that contributes to creating a positive environment include: Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Examples of unacceptable behavior by participants include: The use of sexual imagery and unwelcome sexual advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Our Responsibilities \u00b6 Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful. Scope \u00b6 This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers. Enforcement \u00b6 Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at jailman@schouten-lebbing.nl. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership. Attribution \u00b6 This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq","title":"Code of Conduct"},{"location":"CODE_OF_CONDUCT/#code-of-conduct","text":"","title":"Code of Conduct"},{"location":"CODE_OF_CONDUCT/#our-pledge","text":"In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.","title":"Our Pledge"},{"location":"CODE_OF_CONDUCT/#our-standards","text":"Examples of behavior that contributes to creating a positive environment include: Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Examples of unacceptable behavior by participants include: The use of sexual imagery and unwelcome sexual advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission","title":"Our Standards"},{"location":"CODE_OF_CONDUCT/#our-responsibilities","text":"Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.","title":"Our Responsibilities"},{"location":"CODE_OF_CONDUCT/#scope","text":"This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.","title":"Scope"},{"location":"CODE_OF_CONDUCT/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at jailman@schouten-lebbing.nl. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.","title":"Enforcement"},{"location":"CODE_OF_CONDUCT/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq","title":"Attribution"},{"location":"CONTRIBUTING/","text":"Contribution and Review Guidelines \u00b6 This project welcomes any and all input, but we need to have a few quality guidelines. These guidelines will be examplained here, in this document. GIT Guidelines \u00b6 New to GIT \u00b6 If you have never used git before, you can look up our general reference on our wiki. Git and You \u00b6 GIT is a fantastic system, but while using it we have a few guidelines to keep it fantastic for everyone. Submit complete PR's. Add [DNM] if you do not want your PR merged yet. Always try and fill in the whole form, even for small PR's. Don't close when a reviewer requests changes (just push the changes or ask for help). Explain what you did in your PR. Be thorough. If you can add screenshots to clarify. Always try to add \"Fixes #000\" (where 000 is the Issue your PR fixes) found something you want to fix yourself? Please do make an issue too. Structure Guidelines \u00b6 Naming scheme \u00b6 File and folder names are important and making mistakes in them may give conflicts an/or annoyance in the future. Remember, your garbage needs to be cleaned by someone sometime in the future! For that reason, we have a few guidelines in regards to naming files and folder. Always start files and folders WITHOUT a Capital. Inclusion of files and folders \u00b6 Although GIT is quite friendly in what it accepts in terms of files and folder changes in a commit, a reviewer's or bugfixer's time is not unlimited. For that reason, we have a few specific guidelines in regards to the inclusion of files and folders in your PR. Only include files you actually changed. Try not to include multiple changes in one PR Want to change the formatting of multiple files too? Make a separate PR. Always include the following files when creating a new jail install.sh update.sh readme.md config.yml Code Guidelines \u00b6 Your code, your style, my review \u00b6 Here at jailman, we value people having their own style. But your code needs to be reviewable and editable by others too. For that reason, we have a few basic coding guidelines Always explain regex in a comment within your code. Write simple code and don't try to impress. We will run (Basic) automated reformating of code once in a while. Document your changes in your code and if need be, on the wiki. All PR's should be able to pass our automated shellcheck. It's okey to add shellcheck ignores, but only AFTER you checked the warning! jail requirements \u00b6 Jails should always save user-specific data in a persistant location. Which is the location specified in the config.yml file under \"config:\", which is automatically mounted to every jail under /config. There should be no user specific data in the jail itself Jails should not require the user to edit any config file themselves. All config changes should be automated Jails should not use default passwords, the user should always be forced(!) to put credentials in config.yml manually Review Guidelines \u00b6 Even us review gods need some guidelines once in a while. Let people learn from their mistakes Review instead of merging without comments Abide by these guidelines in your review Tests exist for a reason. Don't merge with test-failures Todo vs Feature vs bug: \u00b6 Please take note of the difference between a TODO and Feature Bug: An unexpected behavior of the script or a crash. Including, but not limited to, errors and warnings. Todo: When you come across something that needs tweaking/adding during development, is not an unexpected behavior Feature: When you, out of personal preference, want something added or changed. That's it! \u00b6 Someone will come along and review the changes. If everything looks good then they will merge it with the main repo. If you need any help don't be afraid to ask in the discord channel: https://discord.gg/tFcTpBp","title":"Contribution and Review Guidelines"},{"location":"CONTRIBUTING/#contribution-and-review-guidelines","text":"This project welcomes any and all input, but we need to have a few quality guidelines. These guidelines will be examplained here, in this document.","title":"Contribution and Review Guidelines"},{"location":"CONTRIBUTING/#git-guidelines","text":"","title":"GIT Guidelines"},{"location":"CONTRIBUTING/#new-to-git","text":"If you have never used git before, you can look up our general reference on our wiki.","title":"New to GIT"},{"location":"CONTRIBUTING/#git-and-you","text":"GIT is a fantastic system, but while using it we have a few guidelines to keep it fantastic for everyone. Submit complete PR's. Add [DNM] if you do not want your PR merged yet. Always try and fill in the whole form, even for small PR's. Don't close when a reviewer requests changes (just push the changes or ask for help). Explain what you did in your PR. Be thorough. If you can add screenshots to clarify. Always try to add \"Fixes #000\" (where 000 is the Issue your PR fixes) found something you want to fix yourself? Please do make an issue too.","title":"Git and You"},{"location":"CONTRIBUTING/#structure-guidelines","text":"","title":"Structure Guidelines"},{"location":"CONTRIBUTING/#naming-scheme","text":"File and folder names are important and making mistakes in them may give conflicts an/or annoyance in the future. Remember, your garbage needs to be cleaned by someone sometime in the future! For that reason, we have a few guidelines in regards to naming files and folder. Always start files and folders WITHOUT a Capital.","title":"Naming scheme"},{"location":"CONTRIBUTING/#inclusion-of-files-and-folders","text":"Although GIT is quite friendly in what it accepts in terms of files and folder changes in a commit, a reviewer's or bugfixer's time is not unlimited. For that reason, we have a few specific guidelines in regards to the inclusion of files and folders in your PR. Only include files you actually changed. Try not to include multiple changes in one PR Want to change the formatting of multiple files too? Make a separate PR. Always include the following files when creating a new jail install.sh update.sh readme.md config.yml","title":"Inclusion of files and folders"},{"location":"CONTRIBUTING/#code-guidelines","text":"","title":"Code Guidelines"},{"location":"CONTRIBUTING/#your-code-your-style-my-review","text":"Here at jailman, we value people having their own style. But your code needs to be reviewable and editable by others too. For that reason, we have a few basic coding guidelines Always explain regex in a comment within your code. Write simple code and don't try to impress. We will run (Basic) automated reformating of code once in a while. Document your changes in your code and if need be, on the wiki. All PR's should be able to pass our automated shellcheck. It's okey to add shellcheck ignores, but only AFTER you checked the warning!","title":"Your code, your style, my review"},{"location":"CONTRIBUTING/#jail-requirements","text":"Jails should always save user-specific data in a persistant location. Which is the location specified in the config.yml file under \"config:\", which is automatically mounted to every jail under /config. There should be no user specific data in the jail itself Jails should not require the user to edit any config file themselves. All config changes should be automated Jails should not use default passwords, the user should always be forced(!) to put credentials in config.yml manually","title":"jail requirements"},{"location":"CONTRIBUTING/#review-guidelines","text":"Even us review gods need some guidelines once in a while. Let people learn from their mistakes Review instead of merging without comments Abide by these guidelines in your review Tests exist for a reason. Don't merge with test-failures","title":"Review Guidelines"},{"location":"CONTRIBUTING/#todo-vs-feature-vs-bug","text":"Please take note of the difference between a TODO and Feature Bug: An unexpected behavior of the script or a crash. Including, but not limited to, errors and warnings. Todo: When you come across something that needs tweaking/adding during development, is not an unexpected behavior Feature: When you, out of personal preference, want something added or changed.","title":"Todo vs Feature vs bug:"},{"location":"CONTRIBUTING/#thats-it","text":"Someone will come along and review the changes. If everything looks good then they will merge it with the main repo. If you need any help don't be afraid to ask in the discord channel: https://discord.gg/tFcTpBp","title":"That's it!"},{"location":"config%20options/","text":"Config Options \u00b6 There are a lot of possibly configuration options in config.yml. For jail specific config options, please see the wiki documentation for your specific jail. This page only list general and global config options, that are the same for every jail. Global config options \u00b6 Global options apply to every jail. Use and change with caution. dataset \u00b6 All config options under \"dataset\" change dataset creation and linking. The indentation and \"dataset\" flag are not optional. All Datasets are auto-created if they do not exist already, no need to worry about creating them! config: The dataset that is going to contain the persistant data for every jail. For example: Nextcloud user files for nextcloud or the actual database for mariadb. iocage: The dataset containing the iocage config. In FreeNAS often poolname/iocage media: The dataset that is going to contain all media files for plex, Sonarr, Radarr etc. Such as movies and music. Music, Movie etc. sub-datasets are auto-created. downloads: The dataset containging temporary download files. These are moved to media when finished. complete, incomplete etc. sub-datasets are auto-created. jails \u00b6 All config options under \"jails\" change default jail settings that are the same for every created jail. The indentation and \"jails\" flag are not optional. - version: the current to-be-installed version for jails - pkgs: packages that are installed to all created jails (hidden) Auto created datasets \u00b6 Some datasets are auto created and can not be changed from the config file. This is done to ease troubleshooting. media/music created as a sub-dataset of media, contains music media/movies created as a sub-dataset of media, contains movies media/shows created as a sub-dataset of media, contains tv-shows downloads/complete created as a sub-dataset of downloads, contains completed downloads downloads/incomplete created as a sub-dataset of downloads, contains not-yet-completed downloads General config options \u00b6 Networking \u00b6 Please be aware that dhcp is not actively supported, many of the jails depend on having a fixed IP-adress in the config file. Some also depend on other jails having a fixed IP in the config file. Use of DHCP is on your own risk and might not work. ip4_addr: To set a static IP (recommended), enter the desired ip address here. Leave blank (or remove the line) for DHCP. gateway: Set the gateway IP for static IP setup. Leave blank (or remove the line) for DHCP. Advanced \u00b6 interfaces: Set the \"interfaces\" flag for iocage. Example: vnet0:bridge0 (optional) dhcp: Set to \"on\" to force DHCP (not required for DHCP, see above) pkgs: Override the to-be-install packages for this jail (might break now or break updates)","title":"Config options"},{"location":"config%20options/#config-options","text":"There are a lot of possibly configuration options in config.yml. For jail specific config options, please see the wiki documentation for your specific jail. This page only list general and global config options, that are the same for every jail.","title":"Config Options"},{"location":"config%20options/#global-config-options","text":"Global options apply to every jail. Use and change with caution.","title":"Global config options"},{"location":"config%20options/#dataset","text":"All config options under \"dataset\" change dataset creation and linking. The indentation and \"dataset\" flag are not optional. All Datasets are auto-created if they do not exist already, no need to worry about creating them! config: The dataset that is going to contain the persistant data for every jail. For example: Nextcloud user files for nextcloud or the actual database for mariadb. iocage: The dataset containing the iocage config. In FreeNAS often poolname/iocage media: The dataset that is going to contain all media files for plex, Sonarr, Radarr etc. Such as movies and music. Music, Movie etc. sub-datasets are auto-created. downloads: The dataset containging temporary download files. These are moved to media when finished. complete, incomplete etc. sub-datasets are auto-created.","title":"dataset"},{"location":"config%20options/#jails","text":"All config options under \"jails\" change default jail settings that are the same for every created jail. The indentation and \"jails\" flag are not optional. - version: the current to-be-installed version for jails - pkgs: packages that are installed to all created jails","title":"jails"},{"location":"config%20options/#hidden-auto-created-datasets","text":"Some datasets are auto created and can not be changed from the config file. This is done to ease troubleshooting. media/music created as a sub-dataset of media, contains music media/movies created as a sub-dataset of media, contains movies media/shows created as a sub-dataset of media, contains tv-shows downloads/complete created as a sub-dataset of downloads, contains completed downloads downloads/incomplete created as a sub-dataset of downloads, contains not-yet-completed downloads","title":"(hidden) Auto created datasets"},{"location":"config%20options/#general-config-options","text":"","title":"General config options"},{"location":"config%20options/#networking","text":"Please be aware that dhcp is not actively supported, many of the jails depend on having a fixed IP-adress in the config file. Some also depend on other jails having a fixed IP in the config file. Use of DHCP is on your own risk and might not work. ip4_addr: To set a static IP (recommended), enter the desired ip address here. Leave blank (or remove the line) for DHCP. gateway: Set the gateway IP for static IP setup. Leave blank (or remove the line) for DHCP.","title":"Networking"},{"location":"config%20options/#advanced","text":"interfaces: Set the \"interfaces\" flag for iocage. Example: vnet0:bridge0 (optional) dhcp: Set to \"on\" to force DHCP (not required for DHCP, see above) pkgs: Override the to-be-install packages for this jail (might break now or break updates)","title":"Advanced"},{"location":"blueprints/bitwarden/","text":"Bitwarden_RS \u00b6 Original README from the Bitwarden_rs github: \u00b6 https://github.com/dani-garcia/bitwarden_rs Bitwarden_RS \u00b6 This is a Bitwarden server API implementation written in Rust compatible with upstream Bitwarden clients *, perfect for self-hosted deployment where running the official resource-heavy service might not be ideal. \u00b6 Image is based on Rust implementation of Bitwarden API . This project is not associated with the Bitwarden project nor 8bit Solutions LLC. \u26a0\ufe0f IMPORTANT \u26a0\ufe0f: When using this server, please report any bugs or suggestions to us directly (look at the bottom of this page for ways to get in touch), regardless of whatever clients you are using (mobile, desktop, browser...). DO NOT use the official support channels. \u00b6 Features \u00b6 Basically full implementation of Bitwarden API is provided including: Single user functionality Organizations support Attachments Vault API support Serving the static files for Vault interface Website icons API Authenticator and U2F support YubiKey and Duo support Installation \u00b6 Pull the docker image and mount a volume from the host for persistent storage: docker pull bitwardenrs/server:latest docker run -d --name bitwarden -v /bw-data/:/data/ -p 80 :80 bitwardenrs/server:latest This will preserve any persistent data under /bw-data/, you can adapt the path to whatever suits you. IMPORTANT : Some web browsers, like Chrome, disallow the use of Web Crypto APIs in insecure contexts. In this case, you might get an error like Cannot read property 'importKey' . To solve this problem, you need to access the web vault from HTTPS. This can be configured in bitwarden_rs directly or using a third-party reverse proxy ( some examples ). If you have an available domain name, you can get HTTPS certificates with Let's Encrypt , or you can generate self-signed certificates with utilities like mkcert . Some proxies automatically do this step, like Caddy (see examples linked above). Usage \u00b6 See the bitwarden_rs wiki for more information on how to configure and run the bitwarden_rs server. Get in touch \u00b6 To ask a question, offer suggestions or new features or to get help configuring or installing the software, please use the forum . If you spot any bugs or crashes with bitwarden_rs itself, please create an issue . Make sure there aren't any similar issues open, though! If you prefer to chat, we're usually hanging around at #bitwarden_rs:matrix.org room on Matrix. Feel free to join us! Sponsors \u00b6 Thanks for your contribution to the project! @ChonoN","title":"Bitwarden_RS"},{"location":"blueprints/bitwarden/#bitwarden_rs","text":"","title":"Bitwarden_RS"},{"location":"blueprints/bitwarden/#original-readme-from-the-bitwarden_rs-github","text":"https://github.com/dani-garcia/bitwarden_rs","title":"Original README from the Bitwarden_rs github:"},{"location":"blueprints/bitwarden/#bitwarden_rs_1","text":"","title":"Bitwarden_RS"},{"location":"blueprints/bitwarden/#this-is-a-bitwarden-server-api-implementation-written-in-rust-compatible-with-upstream-bitwarden-clients-perfect-for-self-hosted-deployment-where-running-the-official-resource-heavy-service-might-not-be-ideal","text":"Image is based on Rust implementation of Bitwarden API . This project is not associated with the Bitwarden project nor 8bit Solutions LLC.","title":"This is a Bitwarden server API implementation written in Rust compatible with upstream Bitwarden clients*, perfect for self-hosted deployment where running the official resource-heavy service might not be ideal."},{"location":"blueprints/bitwarden/#important-when-using-this-server-please-report-any-bugs-or-suggestions-to-us-directly-look-at-the-bottom-of-this-page-for-ways-to-get-in-touch-regardless-of-whatever-clients-you-are-using-mobile-desktop-browser-do-not-use-the-official-support-channels","text":"","title":"\u26a0\ufe0fIMPORTANT\u26a0\ufe0f: When using this server, please report any bugs or suggestions to us directly (look at the bottom of this page for ways to get in touch), regardless of whatever clients you are using (mobile, desktop, browser...). DO NOT use the official support channels."},{"location":"blueprints/bitwarden/#features","text":"Basically full implementation of Bitwarden API is provided including: Single user functionality Organizations support Attachments Vault API support Serving the static files for Vault interface Website icons API Authenticator and U2F support YubiKey and Duo support","title":"Features"},{"location":"blueprints/bitwarden/#installation","text":"Pull the docker image and mount a volume from the host for persistent storage: docker pull bitwardenrs/server:latest docker run -d --name bitwarden -v /bw-data/:/data/ -p 80 :80 bitwardenrs/server:latest This will preserve any persistent data under /bw-data/, you can adapt the path to whatever suits you. IMPORTANT : Some web browsers, like Chrome, disallow the use of Web Crypto APIs in insecure contexts. In this case, you might get an error like Cannot read property 'importKey' . To solve this problem, you need to access the web vault from HTTPS. This can be configured in bitwarden_rs directly or using a third-party reverse proxy ( some examples ). If you have an available domain name, you can get HTTPS certificates with Let's Encrypt , or you can generate self-signed certificates with utilities like mkcert . Some proxies automatically do this step, like Caddy (see examples linked above).","title":"Installation"},{"location":"blueprints/bitwarden/#usage","text":"See the bitwarden_rs wiki for more information on how to configure and run the bitwarden_rs server.","title":"Usage"},{"location":"blueprints/bitwarden/#get-in-touch","text":"To ask a question, offer suggestions or new features or to get help configuring or installing the software, please use the forum . If you spot any bugs or crashes with bitwarden_rs itself, please create an issue . Make sure there aren't any similar issues open, though! If you prefer to chat, we're usually hanging around at #bitwarden_rs:matrix.org room on Matrix. Feel free to join us!","title":"Get in touch"},{"location":"blueprints/bitwarden/#sponsors","text":"Thanks for your contribution to the project! @ChonoN","title":"Sponsors"},{"location":"blueprints/influxdb/","text":"InfluxDB \u00b6 Original README from the influxdb github: \u00b6 https://github.com/influxdata/influxdb InfluxDB \u00b6 InfluxDB is an open source time series platform. This includes APIs for storing and querying data, processing it in the background for ETL or monitoring and alerting purposes, user dashboards, and visualizing and exploring the data and more. The master branch on this repo now represents the latest InfluxDB, which now includes functionality for Kapacitor (background processing) and Chronograf (the UI) all in a single binary. The list of InfluxDB Client Libraries that are compatible with the latest version can be found in our documentation . If you are looking for the 1.x line of releases, there are branches for each minor version as well as a master-1.x branch that will contain the code for the next 1.x release. The master-1.x working branch is here . The InfluxDB 1.x Go Client can be found here . State of the Project \u00b6 The latest InfluxDB 1.x is the stable release and recommended for production use. The InfluxDB that is on the master branch is currently in the beta stage. This means that it is still NOT recommended for production usage. There may be breaking API changes, breaking changes in the Flux language , changes in the underlying storage format that will require you to delete all your data, and significant changes to the UI. The beta is intended for feature exploration and gathering feedback on the available feature set. It SHOULD NOT be used for performance testing, benchmarks, or other stress tests. Additional features will arrive during the beta period until we reach general availability (GA). We will be cutting versioned releases at least every two weeks starting in the first release. There will also be nightly builds based off the latest code in master. Once we close on the final feature set of what will be in the first GA release of InfluxDB in the 2.x line, we will move into the release candidate (RC) phase. At that point, we do not expect there to be breaking changes to the API or Flux language. We may still need to make a breaking change prior to GA due to some unforseen circumstance, but it would need to be extremely important and will be clearly communicated via the changelog and all available channels. Our current plans are to release RCs suitable for production usage, but we will re-evaluate in consultation with the community as the cycle progresses. During the RC period, we will focus on feedback from users, bug fixes, performance, and additive features (where time permits). What you can expect in the Beta and RC Phases \u00b6 Beta \u00b6 Releases every two weeks or as needed Planned additions include: - Compatibility layer with 1.x including: 1.x HTTP Write API and HTTP Read API support for InfluxQL - Import Bulk Data from 1.x - convert TSM from 1.x to 2.x - Performance tuning, stability improvements, and fine tuning based on community feedback. - Finalization of supported client libraries starting with JavaScript and Go. RC \u00b6 As needed Planned activities include: - Performance tuning, stability improvements, and fine-tuning based on community feedback. What is NOT planned? \u00b6 Migration of users/security permissions from InfluxDB v1.x to 2.x. ACTION REQUIRED: Re-establish users and permissions within the new unified security model which now spans the underlying database and user interface. Migration of Continuous Queries. ACTION REQUIRED: These will need to be re-implemented as Flux tasks. Direct support by InfluxDB for CollectD, StatsD, Graphite, or UDP. ACTION REQUIRED: Leverage Telegraf 1.9+ along with the InfluxDB v2.0 output plugin to translate these protocols/formats. Installing from Source \u00b6 We have nightly and weekly versioned Docker images, Debian packages, RPM packages, and tarballs of InfluxDB available at the InfluxData downloads page . Building From Source \u00b6 This project requires Go 1.13 and Go module support. Set GO111MODULE=on or build the project outside of your GOPATH for it to succeed. The project also requires a recent stable version of Rust. We recommend using rustup to install Rust. If you are getting an error loading module requirements error with bzr executable file not found in $PATH\u201d on make , then you need to ensure you have bazaar , protobuf , and yarn installed. OSX: brew install bazaar yarn Linux (Arch): pacman -S bzr protobuf yarn Linux (Ubuntu): apt install bzr protobuf-compiler yarnpkg NB: For RedHat, there are some extra steps: You must enable the EPEL You must add the yarn repository For information about modules, please refer to the wiki . A successful make run results in two binaries, with platform-dependent paths: $ make ... env GO111MODULE=on go build -tags 'assets ' -o bin/$(uname -s | tr '[:upper:]' '[:lower:]')/influx ./cmd/influx env GO111MODULE=on go build -tags 'assets ' -o bin/$(uname -s | tr '[:upper:]' '[:lower:]')/influxd ./cmd/influxd influxd is the InfluxDB service. influx is the CLI management tool. Start the service. Logs to stdout by default: $ bin/$(uname -s | tr '[:upper:]' '[:lower:]')/influxd Getting Started \u00b6 For a complete getting started guide, please see our full online documentation site . To write and query data or use the API in any way, you'll need to first create a user, credentials, organization and bucket. Everything in InfluxDB is organized under a concept of an organization. The API is designed to be multi-tenant. Buckets represent where you store time series data. They're synonymous with what was previously in InfluxDB 1.x a database and retention policy. The simplest way to get set up is to point your browser to http://localhost:9999 and go through the prompts. Note : Port 9999 will be used during the beta phases of development of InfluxDB v2.0. This should allow a v2.0-beta instance to be run alongside a v1.x instance without interfering on port 8086. InfluxDB will thereafter continue to use 8086. You can also get set up from the CLI using the subcommands influx user , influx auth , influx org and influx bucket , or do it all in one breath with influx setup : $ bin/$(uname -s | tr '[:upper:]' '[:lower:]')/influx setup Welcome to InfluxDB 2.0! Please type your primary username: marty Please type your password: Please type your password again: Please type your primary organization name.: InfluxData Please type your primary bucket name.: telegraf Please type your retention period in hours. Or press ENTER for infinite.: 72 You have entered: Username: marty Organization: InfluxData Bucket: telegraf Retention Period: 72 hrs Confirm? (y/n): y UserID Username Organization Bucket 033a3f2c5ccaa000 marty InfluxData Telegraf Your token has been stored in /Users/marty/.influxdbv2/credentials You may get into a development loop where influx setup becomes tedious. Some added flags can help: $ bin/$(uname -s | tr '[:upper:]' '[:lower:]')/influx setup --username marty --password F1uxKapacit0r85 --org InfluxData --bucket telegraf --retention 168 --token where-were-going-we-dont-need-roads --force ~/.influxdbv2/credentials contains your auth token. Most influx commands read the token from this file path by default. You may need the organization ID and bucket ID later: $ influx org find ID Name 033a3f2c708aa000 InfluxData $ influx bucket find ID Name Retention Organization OrganizationID 033a3f2c710aa000 telegraf 72h0m0s InfluxData 033a3f2c708aa000 Write to measurement m , with tag v=2 , in bucket telegraf , which belongs to organization InfluxData : $ bin/$(uname -s | tr '[:upper:]' '[:lower:]')/influx write --org InfluxData --bucket telegraf --precision s \"m v=2 $(date +%s)\" Write the same point using curl : curl --header \"Authorization: Token $(cat ~/.influxdbv2/credentials)\" --data-raw \"m v=2 $(date +%s)\" \"http://localhost:9999/api/v2/write?org=InfluxData&bucket=telegraf&precision=s\" Read that back with a simple Flux query: $ bin/$(uname -s | tr '[:upper:]' '[:lower:]')/influx query -o InfluxData 'from(bucket:\"telegraf\") |> range(start:-1h)' Result: _result Table: keys: [_start, _stop, _field, _measurement] _start:time _stop:time _field:string _measurement:string _time:time _value:float ------------------------------ ------------------------------ ---------------------- ---------------------- ------------------------------ ---------------------------- 2019-12-30T22:19:39.043918000Z 2019-12-30T23:19:39.043918000Z v m 2019-12-30T23:17:02.000000000Z 2 Use the fancy REPL: $ bin/$(uname -s | tr '[:upper:]' '[:lower:]')/influx repl -o InfluxData > from(bucket:\"telegraf\") |> range(start:-1h) Result: _result Table: keys: [_start, _stop, _field, _measurement] _start:time _stop:time _field:string _measurement:string _time:time _value:float ------------------------------ ------------------------------ ---------------------- ---------------------- ------------------------------ ---------------------------- 2019-12-30T22:22:44.776351000Z 2019-12-30T23:22:44.776351000Z v m 2019-12-30T23:17:02.000000000Z 2 > Introducing Flux \u00b6 Flux is an MIT-licensed data scripting language (previously named IFQL) used for querying time series data from InfluxDB. The source for Flux is available on GitHub . Learn more about Flux from CTO Paul Dix's presentation . Contributing to the Project \u00b6 InfluxDB is an MIT licensed open source project and we love our community. The fastest way to get something fixed is to open a PR. Check out our contributing guide if you're interested in helping out. Also, join us on our Community Slack Workspace if you have questions or comments for our engineering teams. CI and Static Analysis \u00b6 CI \u00b6 All pull requests will run through CI, which is currently hosted by Circle. Community contributors should be able to see the outcome of this process by looking at the checks on their PR. Please fix any issues to ensure a prompt review from members of the team. The InfluxDB project is used internally in a number of proprietary InfluxData products, and as such, PRs and changes need to be tested internally. This can take some time, and is not really visible to community contributors. Static Analysis \u00b6 This project uses the following static analysis tools. Failure during the running of any of these tools results in a failed build. Generally, code must be adjusted to satisfy these tools, though there are exceptions. go vet checks for Go code that should be considered incorrect. go fmt checks that Go code is correctly formatted. go mod tidy ensures that the source code and go.mod agree. staticcheck checks for things like: unused code, code that can be simplified, code that is incorrect and code that will have performance issues. staticcheck \u00b6 If your PR fails staticcheck it is easy to dig into why it failed, and also to fix the problem. First, take a look at the error message in Circle under the staticcheck build section, e.g., tsdb/tsm1/encoding.gen.go:1445:24: func BooleanValues.assertOrdered is unused (U1000) tsdb/tsm1/encoding.go:172:7: receiver name should not be an underscore, omit the name if it is unused (ST1006) Next, go and take a look here for some clarification on the error code that you have received, e.g., U1000 . The docs will tell you what's wrong, and often what you need to do to fix the issue. Generated Code \u00b6 Sometimes generated code will contain unused code or occasionally that will fail a different check. staticcheck allows for entire files to be ignored, though it's not ideal. A linter directive, in the form of a comment, must be placed within the generated file. This is problematic because it will be erased if the file is re-generated. Until a better solution comes about, below is the list of generated files that need an ignores comment. If you re-generate a file and find that staticcheck has failed, please see this list below for what you need to put back: File Comment query/promql/promql.go //lint:file-ignore SA6001 Ignore all unused code, it's generated End-to-End Tests \u00b6 CI also runs end-to-end tests. These test the integration between the influx server the ui. You can run them locally in two steps: Start the server in \"testing mode\" by running make run-e2e . Run the tests with make e2e .","title":"InfluxDB"},{"location":"blueprints/influxdb/#influxdb","text":"","title":"InfluxDB"},{"location":"blueprints/influxdb/#original-readme-from-the-influxdb-github","text":"https://github.com/influxdata/influxdb","title":"Original README from the influxdb github:"},{"location":"blueprints/influxdb/#influxdb_1","text":"InfluxDB is an open source time series platform. This includes APIs for storing and querying data, processing it in the background for ETL or monitoring and alerting purposes, user dashboards, and visualizing and exploring the data and more. The master branch on this repo now represents the latest InfluxDB, which now includes functionality for Kapacitor (background processing) and Chronograf (the UI) all in a single binary. The list of InfluxDB Client Libraries that are compatible with the latest version can be found in our documentation . If you are looking for the 1.x line of releases, there are branches for each minor version as well as a master-1.x branch that will contain the code for the next 1.x release. The master-1.x working branch is here . The InfluxDB 1.x Go Client can be found here .","title":"InfluxDB"},{"location":"blueprints/influxdb/#state-of-the-project","text":"The latest InfluxDB 1.x is the stable release and recommended for production use. The InfluxDB that is on the master branch is currently in the beta stage. This means that it is still NOT recommended for production usage. There may be breaking API changes, breaking changes in the Flux language , changes in the underlying storage format that will require you to delete all your data, and significant changes to the UI. The beta is intended for feature exploration and gathering feedback on the available feature set. It SHOULD NOT be used for performance testing, benchmarks, or other stress tests. Additional features will arrive during the beta period until we reach general availability (GA). We will be cutting versioned releases at least every two weeks starting in the first release. There will also be nightly builds based off the latest code in master. Once we close on the final feature set of what will be in the first GA release of InfluxDB in the 2.x line, we will move into the release candidate (RC) phase. At that point, we do not expect there to be breaking changes to the API or Flux language. We may still need to make a breaking change prior to GA due to some unforseen circumstance, but it would need to be extremely important and will be clearly communicated via the changelog and all available channels. Our current plans are to release RCs suitable for production usage, but we will re-evaluate in consultation with the community as the cycle progresses. During the RC period, we will focus on feedback from users, bug fixes, performance, and additive features (where time permits).","title":"State of the Project"},{"location":"blueprints/influxdb/#what-you-can-expect-in-the-beta-and-rc-phases","text":"","title":"What you can expect in the Beta and RC Phases"},{"location":"blueprints/influxdb/#beta","text":"Releases every two weeks or as needed Planned additions include: - Compatibility layer with 1.x including: 1.x HTTP Write API and HTTP Read API support for InfluxQL - Import Bulk Data from 1.x - convert TSM from 1.x to 2.x - Performance tuning, stability improvements, and fine tuning based on community feedback. - Finalization of supported client libraries starting with JavaScript and Go.","title":"Beta"},{"location":"blueprints/influxdb/#rc","text":"As needed Planned activities include: - Performance tuning, stability improvements, and fine-tuning based on community feedback.","title":"RC"},{"location":"blueprints/influxdb/#what-is-not-planned","text":"Migration of users/security permissions from InfluxDB v1.x to 2.x. ACTION REQUIRED: Re-establish users and permissions within the new unified security model which now spans the underlying database and user interface. Migration of Continuous Queries. ACTION REQUIRED: These will need to be re-implemented as Flux tasks. Direct support by InfluxDB for CollectD, StatsD, Graphite, or UDP. ACTION REQUIRED: Leverage Telegraf 1.9+ along with the InfluxDB v2.0 output plugin to translate these protocols/formats.","title":"What is NOT planned?"},{"location":"blueprints/influxdb/#installing-from-source","text":"We have nightly and weekly versioned Docker images, Debian packages, RPM packages, and tarballs of InfluxDB available at the InfluxData downloads page .","title":"Installing from Source"},{"location":"blueprints/influxdb/#building-from-source","text":"This project requires Go 1.13 and Go module support. Set GO111MODULE=on or build the project outside of your GOPATH for it to succeed. The project also requires a recent stable version of Rust. We recommend using rustup to install Rust. If you are getting an error loading module requirements error with bzr executable file not found in $PATH\u201d on make , then you need to ensure you have bazaar , protobuf , and yarn installed. OSX: brew install bazaar yarn Linux (Arch): pacman -S bzr protobuf yarn Linux (Ubuntu): apt install bzr protobuf-compiler yarnpkg NB: For RedHat, there are some extra steps: You must enable the EPEL You must add the yarn repository For information about modules, please refer to the wiki . A successful make run results in two binaries, with platform-dependent paths: $ make ... env GO111MODULE=on go build -tags 'assets ' -o bin/$(uname -s | tr '[:upper:]' '[:lower:]')/influx ./cmd/influx env GO111MODULE=on go build -tags 'assets ' -o bin/$(uname -s | tr '[:upper:]' '[:lower:]')/influxd ./cmd/influxd influxd is the InfluxDB service. influx is the CLI management tool. Start the service. Logs to stdout by default: $ bin/$(uname -s | tr '[:upper:]' '[:lower:]')/influxd","title":"Building From Source"},{"location":"blueprints/influxdb/#getting-started","text":"For a complete getting started guide, please see our full online documentation site . To write and query data or use the API in any way, you'll need to first create a user, credentials, organization and bucket. Everything in InfluxDB is organized under a concept of an organization. The API is designed to be multi-tenant. Buckets represent where you store time series data. They're synonymous with what was previously in InfluxDB 1.x a database and retention policy. The simplest way to get set up is to point your browser to http://localhost:9999 and go through the prompts. Note : Port 9999 will be used during the beta phases of development of InfluxDB v2.0. This should allow a v2.0-beta instance to be run alongside a v1.x instance without interfering on port 8086. InfluxDB will thereafter continue to use 8086. You can also get set up from the CLI using the subcommands influx user , influx auth , influx org and influx bucket , or do it all in one breath with influx setup : $ bin/$(uname -s | tr '[:upper:]' '[:lower:]')/influx setup Welcome to InfluxDB 2.0! Please type your primary username: marty Please type your password: Please type your password again: Please type your primary organization name.: InfluxData Please type your primary bucket name.: telegraf Please type your retention period in hours. Or press ENTER for infinite.: 72 You have entered: Username: marty Organization: InfluxData Bucket: telegraf Retention Period: 72 hrs Confirm? (y/n): y UserID Username Organization Bucket 033a3f2c5ccaa000 marty InfluxData Telegraf Your token has been stored in /Users/marty/.influxdbv2/credentials You may get into a development loop where influx setup becomes tedious. Some added flags can help: $ bin/$(uname -s | tr '[:upper:]' '[:lower:]')/influx setup --username marty --password F1uxKapacit0r85 --org InfluxData --bucket telegraf --retention 168 --token where-were-going-we-dont-need-roads --force ~/.influxdbv2/credentials contains your auth token. Most influx commands read the token from this file path by default. You may need the organization ID and bucket ID later: $ influx org find ID Name 033a3f2c708aa000 InfluxData $ influx bucket find ID Name Retention Organization OrganizationID 033a3f2c710aa000 telegraf 72h0m0s InfluxData 033a3f2c708aa000 Write to measurement m , with tag v=2 , in bucket telegraf , which belongs to organization InfluxData : $ bin/$(uname -s | tr '[:upper:]' '[:lower:]')/influx write --org InfluxData --bucket telegraf --precision s \"m v=2 $(date +%s)\" Write the same point using curl : curl --header \"Authorization: Token $(cat ~/.influxdbv2/credentials)\" --data-raw \"m v=2 $(date +%s)\" \"http://localhost:9999/api/v2/write?org=InfluxData&bucket=telegraf&precision=s\" Read that back with a simple Flux query: $ bin/$(uname -s | tr '[:upper:]' '[:lower:]')/influx query -o InfluxData 'from(bucket:\"telegraf\") |> range(start:-1h)' Result: _result Table: keys: [_start, _stop, _field, _measurement] _start:time _stop:time _field:string _measurement:string _time:time _value:float ------------------------------ ------------------------------ ---------------------- ---------------------- ------------------------------ ---------------------------- 2019-12-30T22:19:39.043918000Z 2019-12-30T23:19:39.043918000Z v m 2019-12-30T23:17:02.000000000Z 2 Use the fancy REPL: $ bin/$(uname -s | tr '[:upper:]' '[:lower:]')/influx repl -o InfluxData > from(bucket:\"telegraf\") |> range(start:-1h) Result: _result Table: keys: [_start, _stop, _field, _measurement] _start:time _stop:time _field:string _measurement:string _time:time _value:float ------------------------------ ------------------------------ ---------------------- ---------------------- ------------------------------ ---------------------------- 2019-12-30T22:22:44.776351000Z 2019-12-30T23:22:44.776351000Z v m 2019-12-30T23:17:02.000000000Z 2 >","title":"Getting Started"},{"location":"blueprints/influxdb/#introducing-flux","text":"Flux is an MIT-licensed data scripting language (previously named IFQL) used for querying time series data from InfluxDB. The source for Flux is available on GitHub . Learn more about Flux from CTO Paul Dix's presentation .","title":"Introducing Flux"},{"location":"blueprints/influxdb/#contributing-to-the-project","text":"InfluxDB is an MIT licensed open source project and we love our community. The fastest way to get something fixed is to open a PR. Check out our contributing guide if you're interested in helping out. Also, join us on our Community Slack Workspace if you have questions or comments for our engineering teams.","title":"Contributing to the Project"},{"location":"blueprints/influxdb/#ci-and-static-analysis","text":"","title":"CI and Static Analysis"},{"location":"blueprints/influxdb/#ci","text":"All pull requests will run through CI, which is currently hosted by Circle. Community contributors should be able to see the outcome of this process by looking at the checks on their PR. Please fix any issues to ensure a prompt review from members of the team. The InfluxDB project is used internally in a number of proprietary InfluxData products, and as such, PRs and changes need to be tested internally. This can take some time, and is not really visible to community contributors.","title":"CI"},{"location":"blueprints/influxdb/#static-analysis","text":"This project uses the following static analysis tools. Failure during the running of any of these tools results in a failed build. Generally, code must be adjusted to satisfy these tools, though there are exceptions. go vet checks for Go code that should be considered incorrect. go fmt checks that Go code is correctly formatted. go mod tidy ensures that the source code and go.mod agree. staticcheck checks for things like: unused code, code that can be simplified, code that is incorrect and code that will have performance issues.","title":"Static Analysis"},{"location":"blueprints/influxdb/#staticcheck","text":"If your PR fails staticcheck it is easy to dig into why it failed, and also to fix the problem. First, take a look at the error message in Circle under the staticcheck build section, e.g., tsdb/tsm1/encoding.gen.go:1445:24: func BooleanValues.assertOrdered is unused (U1000) tsdb/tsm1/encoding.go:172:7: receiver name should not be an underscore, omit the name if it is unused (ST1006) Next, go and take a look here for some clarification on the error code that you have received, e.g., U1000 . The docs will tell you what's wrong, and often what you need to do to fix the issue.","title":"staticcheck"},{"location":"blueprints/influxdb/#generated-code","text":"Sometimes generated code will contain unused code or occasionally that will fail a different check. staticcheck allows for entire files to be ignored, though it's not ideal. A linter directive, in the form of a comment, must be placed within the generated file. This is problematic because it will be erased if the file is re-generated. Until a better solution comes about, below is the list of generated files that need an ignores comment. If you re-generate a file and find that staticcheck has failed, please see this list below for what you need to put back: File Comment query/promql/promql.go //lint:file-ignore SA6001 Ignore all unused code, it's generated","title":"Generated Code"},{"location":"blueprints/influxdb/#end-to-end-tests","text":"CI also runs end-to-end tests. These test the integration between the influx server the ui. You can run them locally in two steps: Start the server in \"testing mode\" by running make run-e2e . Run the tests with make e2e .","title":"End-to-End Tests"},{"location":"blueprints/jackett/","text":"jackett \u00b6 Jackett is tool designed to combine and search multiple bittorrent trackers like a proxy. Post-install \u00b6 Currently there are not relevant post-install steps, other than your own personal preferences for setting up Jacket.","title":"Jackett"},{"location":"blueprints/jackett/#jackett","text":"Jackett is tool designed to combine and search multiple bittorrent trackers like a proxy.","title":"jackett"},{"location":"blueprints/jackett/#post-install","text":"Currently there are not relevant post-install steps, other than your own personal preferences for setting up Jacket.","title":"Post-install"},{"location":"blueprints/kms/","text":"Py-KMS \u00b6 Original README from the py-kms github: \u00b6 https://github.com/SystemRage/py-kms History \u00b6 py-kms is a port of node-kms created by cyrozap , which is a port of either the C##, C++, or .NET implementations of KMS Emulator. The original version was written by CODYQX4 and is derived from the reverse-engineered code of Microsoft's official KMS. Features \u00b6 Responds to V4, V5, and V6 KMS requests. Supports activating: Windows Vista Windows 7 Windows 8 Windows 8.1 Windows 10 ( 1511 / 1607 / 1703 / 1709 / 1803 / 1809 / 1903 / 1909 ) Windows Server 2008 Windows Server 2008 R2 Windows Server 2012 Windows Server 2012 R2 Windows Server 2016 Windows Server 2019 Microsoft Office 2010 ( Volume License ) Microsoft Office 2013 ( Volume License ) Microsoft Office 2016 ( Volume License ) Microsoft Office 2019 ( Volume License ) It's written in Python: tested with Python 2.7.15rc1 tested with Python 3.6.7 Dependencies \u00b6 Python 3.x or Python 2.7.x or Python 2.6.x with the argparse module installed. Tkinter module. If the tzlocal module is installed, the \"Request Time\" in the verbose output will be converted into local time. Otherwise, it will be in UTC. It can use the sqlite3 module so you can use the database function, storing activation data so it can be recalled again. Installation example on Ubuntu / Mint: sudo apt-get update for python3 sudo apt-get install python3-tk python3-pip sudo pip3 install tzlocal pysqlite3 or for python2 sudo apt-get install python-tk python-pip sudo pip install tzlocal pysqlite Usage \u00b6 NOTE : Pay attention to how invoke scripts, if you want to run with python2 use python... while for python3 use python3... , also depending on the Python versions that resides in your PC. To start the server, execute python pykms_Server.py [IPADDRESS] [PORT] , the default IPADDRESS is \"0.0.0.0\" ( all interfaces ) and the default PORT is \"1688\". To run the client (only for testing purposes), use python pykms_Client.py [IPADDRESS] [PORT] , with the same defaults of pykms_Server.py . To show the help pages type: python pykms_Server.py -h and python pykms_Client.py -h . To generate a random HWID use -w option: python pykms_Server.py -w RANDOM . To get the HWID from any server use the client, for example type: python pykms_Client.py 0.0.0.0 1688 -m Windows8.1 -V INFO . To view a minimal set of logging information use -V MINI option, for example: python pykms_Server.py -F /path/to/your/logfile.log -V MINI . To redirect logging on stdout use -F STDOUT option, for example: python pykms_Server.py -F STDOUT -V DEBUG . You can create logfile and view logging information on stdout at the same time with -F FILESTDOUT option, for example: python pykms_Server.py -F FILESTDOUT /path/to/your/logfile.log -V DEBUG . Select timeout (seconds) for py-kms with -t option, for example python pykms_Server.py -t 10 For launching py-kms GUI make executable pykms_Server.py file with chmod +x /path/to/folder/py-kms/pykms_Server.py , then simply run pykms_Server.py double-clicking. You can run py-kms deamonized (via Etrigan ) using a command like: python pykms_Server.py etrigan start and stop it with: python pykms_Server.py etrigan stop . With Etrigan you have another way to launch py-kms GUI (specially suitable if you're using a virtualenv), so: python pykms_Server.py etrigan start -g and stop the GUI with the same precedent command (or interact with EXIT button). Docker \u00b6 This projects has docker image support. You can find all available image configurations inside the docker folder. There are three tags of the images available: * latest , currently the same like minimal... * minimal , wich is based on the python3 minimal configuration of py-kms. This image does NOT include SQLLite support! * python2 , which is fully configurable and equiped with SQLLite support and web interface. * python3 , which is like the python2 tag - just with Python 3... If you just want to use the image and don't want to build them yourself, you can use the official image at the docker hub ( pykmsorg/py-kms ). To ensure that the image is always up-to-date you should check watchtower out! Other Important Stuff \u00b6 Consult the Wiki for more information about activation with py-kms and to get GVLK keys. License \u00b6","title":"Py-KMS"},{"location":"blueprints/kms/#py-kms","text":"","title":"Py-KMS"},{"location":"blueprints/kms/#original-readme-from-the-py-kms-github","text":"https://github.com/SystemRage/py-kms","title":"Original README from the py-kms github:"},{"location":"blueprints/kms/#history","text":"py-kms is a port of node-kms created by cyrozap , which is a port of either the C##, C++, or .NET implementations of KMS Emulator. The original version was written by CODYQX4 and is derived from the reverse-engineered code of Microsoft's official KMS.","title":"History"},{"location":"blueprints/kms/#features","text":"Responds to V4, V5, and V6 KMS requests. Supports activating: Windows Vista Windows 7 Windows 8 Windows 8.1 Windows 10 ( 1511 / 1607 / 1703 / 1709 / 1803 / 1809 / 1903 / 1909 ) Windows Server 2008 Windows Server 2008 R2 Windows Server 2012 Windows Server 2012 R2 Windows Server 2016 Windows Server 2019 Microsoft Office 2010 ( Volume License ) Microsoft Office 2013 ( Volume License ) Microsoft Office 2016 ( Volume License ) Microsoft Office 2019 ( Volume License ) It's written in Python: tested with Python 2.7.15rc1 tested with Python 3.6.7","title":"Features"},{"location":"blueprints/kms/#dependencies","text":"Python 3.x or Python 2.7.x or Python 2.6.x with the argparse module installed. Tkinter module. If the tzlocal module is installed, the \"Request Time\" in the verbose output will be converted into local time. Otherwise, it will be in UTC. It can use the sqlite3 module so you can use the database function, storing activation data so it can be recalled again. Installation example on Ubuntu / Mint: sudo apt-get update for python3 sudo apt-get install python3-tk python3-pip sudo pip3 install tzlocal pysqlite3 or for python2 sudo apt-get install python-tk python-pip sudo pip install tzlocal pysqlite","title":"Dependencies"},{"location":"blueprints/kms/#usage","text":"NOTE : Pay attention to how invoke scripts, if you want to run with python2 use python... while for python3 use python3... , also depending on the Python versions that resides in your PC. To start the server, execute python pykms_Server.py [IPADDRESS] [PORT] , the default IPADDRESS is \"0.0.0.0\" ( all interfaces ) and the default PORT is \"1688\". To run the client (only for testing purposes), use python pykms_Client.py [IPADDRESS] [PORT] , with the same defaults of pykms_Server.py . To show the help pages type: python pykms_Server.py -h and python pykms_Client.py -h . To generate a random HWID use -w option: python pykms_Server.py -w RANDOM . To get the HWID from any server use the client, for example type: python pykms_Client.py 0.0.0.0 1688 -m Windows8.1 -V INFO . To view a minimal set of logging information use -V MINI option, for example: python pykms_Server.py -F /path/to/your/logfile.log -V MINI . To redirect logging on stdout use -F STDOUT option, for example: python pykms_Server.py -F STDOUT -V DEBUG . You can create logfile and view logging information on stdout at the same time with -F FILESTDOUT option, for example: python pykms_Server.py -F FILESTDOUT /path/to/your/logfile.log -V DEBUG . Select timeout (seconds) for py-kms with -t option, for example python pykms_Server.py -t 10 For launching py-kms GUI make executable pykms_Server.py file with chmod +x /path/to/folder/py-kms/pykms_Server.py , then simply run pykms_Server.py double-clicking. You can run py-kms deamonized (via Etrigan ) using a command like: python pykms_Server.py etrigan start and stop it with: python pykms_Server.py etrigan stop . With Etrigan you have another way to launch py-kms GUI (specially suitable if you're using a virtualenv), so: python pykms_Server.py etrigan start -g and stop the GUI with the same precedent command (or interact with EXIT button).","title":"Usage"},{"location":"blueprints/kms/#docker","text":"This projects has docker image support. You can find all available image configurations inside the docker folder. There are three tags of the images available: * latest , currently the same like minimal... * minimal , wich is based on the python3 minimal configuration of py-kms. This image does NOT include SQLLite support! * python2 , which is fully configurable and equiped with SQLLite support and web interface. * python3 , which is like the python2 tag - just with Python 3... If you just want to use the image and don't want to build them yourself, you can use the official image at the docker hub ( pykmsorg/py-kms ). To ensure that the image is always up-to-date you should check watchtower out!","title":"Docker"},{"location":"blueprints/kms/#other-important-stuff","text":"Consult the Wiki for more information about activation with py-kms and to get GVLK keys.","title":"Other Important Stuff"},{"location":"blueprints/kms/#license","text":"","title":"License"},{"location":"blueprints/lidarr/","text":"Lidarr \u00b6 Original README from the lidarr github: \u00b6 https://github.com/lidarr/Lidarr Lidarr \u00b6 Lidarr is a music collection manager for Usenet and BitTorrent users. It can monitor multiple RSS feeds for new tracks from your favorite artists and will grab, sort and rename them. It can also be configured to automatically upgrade the quality of files already downloaded when a better quality format becomes available. Major Features Include: \u00b6 Support for major platforms: Windows, Linux, macOS, Raspberry Pi, etc. Automatically detects new tracks. Can scan your existing library and download any missing tracks. Can watch for better quality of the tracks you already have and do an automatic upgrade. Automatic failed download handling will try another release if one fails Manual search so you can pick any release or to see why a release was not downloaded automatically Fully configurable track renaming Full integration with SABnzbd and NZBGet Full integration with Kodi, Plex (notification, library update, metadata) Full support for specials and multi-album releases And a beautiful UI Feature Requests \u00b6 Support \u00b6 Contributors \u00b6 This project exists thanks to all the people who contribute. [ Contribute ]. Backers \u00b6 Thank you to all our backers! \ud83d\ude4f [ Become a backer ] Sponsors \u00b6 Support this project by becoming a sponsor. Your logo will show up here with a link to your website. [ Become a sponsor ] License \u00b6 GNU GPL v3 Copyright 2010-2019","title":"Lidarr"},{"location":"blueprints/lidarr/#lidarr","text":"","title":"Lidarr"},{"location":"blueprints/lidarr/#original-readme-from-the-lidarr-github","text":"https://github.com/lidarr/Lidarr","title":"Original README from the lidarr github:"},{"location":"blueprints/lidarr/#lidarr_1","text":"Lidarr is a music collection manager for Usenet and BitTorrent users. It can monitor multiple RSS feeds for new tracks from your favorite artists and will grab, sort and rename them. It can also be configured to automatically upgrade the quality of files already downloaded when a better quality format becomes available.","title":"Lidarr"},{"location":"blueprints/lidarr/#major-features-include","text":"Support for major platforms: Windows, Linux, macOS, Raspberry Pi, etc. Automatically detects new tracks. Can scan your existing library and download any missing tracks. Can watch for better quality of the tracks you already have and do an automatic upgrade. Automatic failed download handling will try another release if one fails Manual search so you can pick any release or to see why a release was not downloaded automatically Fully configurable track renaming Full integration with SABnzbd and NZBGet Full integration with Kodi, Plex (notification, library update, metadata) Full support for specials and multi-album releases And a beautiful UI","title":"Major Features Include:"},{"location":"blueprints/lidarr/#feature-requests","text":"","title":"Feature Requests"},{"location":"blueprints/lidarr/#support","text":"","title":"Support"},{"location":"blueprints/lidarr/#contributors","text":"This project exists thanks to all the people who contribute. [ Contribute ].","title":"Contributors"},{"location":"blueprints/lidarr/#backers","text":"Thank you to all our backers! \ud83d\ude4f [ Become a backer ]","title":"Backers"},{"location":"blueprints/lidarr/#sponsors","text":"Support this project by becoming a sponsor. Your logo will show up here with a link to your website. [ Become a sponsor ]","title":"Sponsors"},{"location":"blueprints/lidarr/#license","text":"GNU GPL v3 Copyright 2010-2019","title":"License"},{"location":"blueprints/mariadb/","text":"MariaDB \u00b6 Original README from the mariadb github: \u00b6 https://github.com/MariaDB/server/ Code status: \u00b6 travis-ci.org (10.5 branch) ci.appveyor.com MariaDB: drop-in replacement for MySQL \u00b6 MariaDB is designed as a drop-in replacement of MySQL(R) with more features, new storage engines, fewer bugs, and better performance. MariaDB is brought to you by the MariaDB Foundation and the MariaDB corporation. Please read the CREDITS file for details about the MariaDB Foundation, and who is developing MariaDB. MariaDB is developed by many of the original developers of MySQL who now work for the MariaDB Corporation, the MariaDB Foundation and by many people in the community. MySQL, which is the base of MariaDB, is a product and trademark of Oracle Corporation, Inc. For a list of developers and other contributors, see the Credits appendix. You can also run 'SHOW authors' to get a list of active contributors. A description of the MariaDB project and a manual can be found at: https://mariadb.com/kb/en/ https://mariadb.com/kb/en/mariadb-vs-mysql-features/ https://mariadb.com/kb/en/mariadb-versus-mysql-compatibility/ https://mariadb.com/kb/en/library/new-and-old-releases/ https://mariadb.org/ As MariaDB is a full replacement of MySQL, the MySQL manual at http://dev.mysql.com/doc is generally applicable. Help \u00b6 More help is available from the Maria Discuss mailing list https://launchpad.net/~maria-discuss and the #maria IRC channel on Freenode. Live QA for beginner contributors \u00b6 MariaDB has a dedicated time each week when we answer new contributor questions live on Zulip and IRC. From 8:00 to 10:00 UTC on Mondays, and 10:00 to 12:00 UTC on Thursdays, anyone can ask any questions they\u2019d like, and a live developer will be available to assist. New contributors can ask questions any time, but we will provide immediate feedback during that interval. Licensing \u00b6 NOTE: MariaDB is specifically available only under version 2 of the GNU General Public License (GPLv2). (I.e. Without the \"any later version\" clause.) This is inherited from MySQL. Please see the README file in the MySQL distribution for more information. License information can be found in the COPYING file. Third party license information can be found in the THIRDPARTY file. Bug Reports \u00b6 Bug and/or error reports regarding MariaDB should be submitted at: https://jira.mariadb.org For reporting security vulnerabilities see: https://mariadb.org/about/security-policy/ Bugs in the MySQL code can also be submitted at: https://bugs.mysql.com The code for MariaDB, including all revision history, can be found at: https://github.com/MariaDB/server","title":"MariaDB"},{"location":"blueprints/mariadb/#mariadb","text":"","title":"MariaDB"},{"location":"blueprints/mariadb/#original-readme-from-the-mariadb-github","text":"https://github.com/MariaDB/server/","title":"Original README from the mariadb github:"},{"location":"blueprints/mariadb/#code-status","text":"travis-ci.org (10.5 branch) ci.appveyor.com","title":"Code status:"},{"location":"blueprints/mariadb/#mariadb-drop-in-replacement-for-mysql","text":"MariaDB is designed as a drop-in replacement of MySQL(R) with more features, new storage engines, fewer bugs, and better performance. MariaDB is brought to you by the MariaDB Foundation and the MariaDB corporation. Please read the CREDITS file for details about the MariaDB Foundation, and who is developing MariaDB. MariaDB is developed by many of the original developers of MySQL who now work for the MariaDB Corporation, the MariaDB Foundation and by many people in the community. MySQL, which is the base of MariaDB, is a product and trademark of Oracle Corporation, Inc. For a list of developers and other contributors, see the Credits appendix. You can also run 'SHOW authors' to get a list of active contributors. A description of the MariaDB project and a manual can be found at: https://mariadb.com/kb/en/ https://mariadb.com/kb/en/mariadb-vs-mysql-features/ https://mariadb.com/kb/en/mariadb-versus-mysql-compatibility/ https://mariadb.com/kb/en/library/new-and-old-releases/ https://mariadb.org/ As MariaDB is a full replacement of MySQL, the MySQL manual at http://dev.mysql.com/doc is generally applicable.","title":"MariaDB: drop-in replacement for MySQL"},{"location":"blueprints/mariadb/#help","text":"More help is available from the Maria Discuss mailing list https://launchpad.net/~maria-discuss and the #maria IRC channel on Freenode.","title":"Help"},{"location":"blueprints/mariadb/#live-qa-for-beginner-contributors","text":"MariaDB has a dedicated time each week when we answer new contributor questions live on Zulip and IRC. From 8:00 to 10:00 UTC on Mondays, and 10:00 to 12:00 UTC on Thursdays, anyone can ask any questions they\u2019d like, and a live developer will be available to assist. New contributors can ask questions any time, but we will provide immediate feedback during that interval.","title":"Live QA for beginner contributors"},{"location":"blueprints/mariadb/#licensing","text":"NOTE: MariaDB is specifically available only under version 2 of the GNU General Public License (GPLv2). (I.e. Without the \"any later version\" clause.) This is inherited from MySQL. Please see the README file in the MySQL distribution for more information. License information can be found in the COPYING file. Third party license information can be found in the THIRDPARTY file.","title":"Licensing"},{"location":"blueprints/mariadb/#bug-reports","text":"Bug and/or error reports regarding MariaDB should be submitted at: https://jira.mariadb.org For reporting security vulnerabilities see: https://mariadb.org/about/security-policy/ Bugs in the MySQL code can also be submitted at: https://bugs.mysql.com The code for MariaDB, including all revision history, can be found at: https://github.com/MariaDB/server","title":"Bug Reports"},{"location":"blueprints/nextcloud/","text":"Nextcloud \u00b6 Original README from the Upstream Nextcloud-iocage install script: \u00b6 https://github.com/danb35/freenas-iocage-nextcloud freenas-iocage-nextcloud \u00b6 Script to create an iocage jail on FreeNAS for the latest Nextcloud 18 release, including Caddy 1.0, MariaDB 10.3/PostgreSQL 10, and Let's Encrypt This script will create an iocage jail on FreeNAS 11.2-U7 or 11.3 with the latest release of Nextcloud 18, along with its dependencies. It will obtain a trusted certificate from Let's Encrypt for the system, install it, and configure it to renew automatically. It will create the Nextcloud database and generate a strong root password and user password for the database system. It will configure the jail to store the database and Nextcloud user data outside the jail, so it will not be lost in the event you need to rebuild the jail. Status \u00b6 This script will work with FreeNAS 11.3, and it should also work with 11.2-U7. Due to the EOL status of FreeBSD 11.2, it is unlikely to work reliably with earlier releases of FreeNAS. Usage \u00b6 Prerequisites (Let's Encrypt) \u00b6 This script works best when your installation is able to obtain a certificate from Let's Encrypt . When you use it this way, Caddy is able to handle all of the TLS-related configuration for you, obtain and renew certificates automatically, etc. In order for this to happen, you must meet the two requirements below: First, you must own or control a real Internet domain name. This script obtains a TLS encryption certificate from Let's Encrypt, who will only issue for public domain names. Thus, domains like cloud.local , mycloud.lan , or nextcloud.home won't work. Domains can be very inexpensive, and in some cases, they can be free. Freenom , for example, provides domains for free if you jump through the right hoops. EasyDNS is a fine domain registrar for paid domains, costing roughly US$15 per year (which varies slightly with the top-level domain). Second, one of these two conditions must be met in order for Let's Encrypt to validate your control over the domain name: You must be able and willing to open ports 80 and 443 from the entire Internet to the jail, and leave them open. If this applies, do it before running this script. DNS hosting for the domain name needs to be with a provider that Caddy supports, to automatically update the DNS records needed to prove your control over the domain. See the Caddy documentation under the heading of \"DNS Providers\" for the supported providers, and what information you'll need in order to proceed. Cloudflare provides DNS hosting at no cost, and it's well-supported by Caddy. Cloudflare doesn't directly provide Dynamic DNS service, but DNS-O-Matic is a Dynamic DNS provider that will interface with many DNS hosts including Cloudflare, and is also free of charge. So, even if you have a dynamic IP address (as most residential Internet users do), you don't have your own domain, and you aren't willing to pay for a domain or any other relevant service, and you aren't willing to open any ports from the Internet to your system, you can still get a trusted certificate from Let's Encrypt by following these steps: Register a free domain with Freenom. Be sure to keep up with the renewal requirements. Sign up for a free account with Cloudflare, and activate it for free DNS service only on your domain. Tell Freenom to use Cloudflare for DNS for your domain. Sign up for a free account with DNS-O-Matic, and configure it to update your Cloudflare DNS. Set up FreeNAS (see this thread ), your router, or whatever you prefer to update DNS-O-Matic as your IP address changes. Set up this script to do DNS validation, tell it to use the cloudflare plugin, and give it your email address and Global API key. If you aren't able or willing to obtain a certificate from Let's Encrypt, this script also supports configuring Caddy with a self-signed certificate, or with no certificate (and thus no HTTPS) at all. Prerequisites (Other) \u00b6 Although not required, it's recommended to create two datasets on your main storage pool: one named files , which will store the Nextcloud user data; and one called db , which will store the SQL database. For optimal performance, set the record size of the db dataset to 16 KB (under Advanced Settings in the FreeNAS web GUI). It's also recommended to cache only metadata on the db dataset; you can do this by running zfs set primarycache=metadata poolname/db . Installation \u00b6 Download the repository to a convenient directory on your FreeNAS system by running git clone https://github.com/danb35/freenas-iocage-nextcloud . Then change into the new directory and create a file called nextcloud-config . It should look like this: JAIL_IP=\"192.168.1.199\" DEFAULT_GW_IP=\"192.168.1.1\" POOL_PATH=\"/mnt/tank\" TIME_ZONE=\"America/New_York\" HOST_NAME=\"YOUR_FQDN\" STANDALONE_CERT=1 CERT_EMAIL=\"me@example.com\" Many of the options are self-explanatory, and all should be adjusted to suit your needs, but only a few are mandatory. The mandatory options are: JAIL_IP is the IP address for your jail DEFAULT_GW_IP is the address for your default gateway POOL_PATH is the path for your data pool. TIME_ZONE is the time zone of your location, in PHP notation--see the PHP manual for a list of all valid time zones. HOST_NAME is the fully-qualified domain name you want to assign to your installation. You must own (or at least control) this domain, because Let's Encrypt will test that control. DNS_CERT, STANDALONE_CERT, SELFSIGNED_CERT, and NO_CERT determine which method will be used to generate a TLS certificate (or, in the case of NO_CERT, indicate that you don't want to use SSL at all). DNS_CERT and STANDALONE_CERT indicate use of DNS or HTTP validation for Let's Encrypt, respectively. One and only one of these must be set to 1. CERT_EMAIL is the email address Let's Encrypt will use to notify you of certificate expiration. This is mandatory regardless of whether you're using Let's Encrypt (Caddy won't start without it), but it's only used with Let's Encrypt. If you are not using one of the Let's Encrypt certificate options, you can set this to a dummy address as above. If you are using Let's Encrypt, though, it should be set to a valid address for the system admin. DNS_PLUGIN: If DNS_CERT is set, DNS_PLUGIN must contain the name of the DNS validation plugin you'll use with Caddy to validate domain control. See the Caddy documentation under the heading of \"DNS Providers\" for the available plugins, but omit the leading \"tls.dns.\". For example, to use Cloudflare, set DNS_PLUGIN=\"cloudflare\" . DNS_ENV: If DNS_CERT is set, DNS_ENV must contain the authentication credentials for your DNS provider. See the Caddy documentation under the heading of \"DNS Providers\" for further details. For Cloudflare, you'd set DNS_ENV=\"CLOUDFLARE_EMAIL=foo@bar.baz CLOUDFLARE_API_KEY=blah\" , using your the email address of your Cloudflare account and your Global API key--the newer API tokens aren't currently supported. In addition, there are some other options which have sensible defaults, but can be adjusted if needed. These are: JAIL_NAME: The name of the jail, defaults to \"nextcloud\" DB_PATH, FILES_PATH, and PORTS_PATH: These are the paths to your database files, your data files, and the FreeBSD Ports collection. They default to $POOL_PATH/db, $POOL_PATH/files, and $POOL_PATH/portsnap, respectively. DATABASE: Which database management system to use. Default is \"mariadb\", but can be set to \"pgsql\" if you prefer to use PostgreSQL. INTERFACE: The network interface to use for the jail. Defaults to vnet0 . VNET: Whether to use the iocage virtual network stack. Defaults to on . If you're going to open ports 80 and 443 from the outside world to your jail, do so before running the script, and set STANDALONE_CERT to 1. If not, but you use a DNS provider that's supported by Caddy, set DNS_CERT to 1. If neither of these is true, you won't be able to use this script without modification. It's also helpful if HOST_NAME resolves to your jail from inside your network. You'll probably need to configure this on your router. If it doesn't, you'll still be able to reach your Nextcloud installation via the jail's IP address, but you'll get certificate errors that way. Execution \u00b6 Once you've downloaded the script and prepared the configuration file, run this script ( ./nextcloud-jail.sh ). The script will run for several minutes. When it finishes, your jail will be created, Nextcloud will be installed and configured, and you'll be shown the randomly-generated password for the default user (\"admin\"). You can then log in and create users, add data, and generally do whatever else you like. Obtaining a trusted Let's Encrypt cert \u00b6 This configuration generated by this script will obtain certs from a non-trusted certificate authority by default. This is to prevent you from exhausting the Let's Encrypt rate limits while you're testing things out. Once you're sure things are working, you'll want to get a trusted cert instead. To do this, you can use a simple script that's included. As long as you haven't changed the default jail name, you can do this by running iocage exec nextcloud /root/remove-staging.sh . To Do \u00b6 I'd appreciate any suggestions (or, better yet, pull requests) to improve the various config files I'm using. Most of them are adapted from the default configuration files that ship with the software in question, and have only been lightly edited to work in this application. But if there are changes to settings or organization that could improve performance, reliability, or security, I'd like to hear about them.","title":"Nextcloud"},{"location":"blueprints/nextcloud/#nextcloud","text":"","title":"Nextcloud"},{"location":"blueprints/nextcloud/#original-readme-from-the-upstream-nextcloud-iocage-install-script","text":"https://github.com/danb35/freenas-iocage-nextcloud","title":"Original README from the Upstream Nextcloud-iocage install script:"},{"location":"blueprints/nextcloud/#freenas-iocage-nextcloud","text":"Script to create an iocage jail on FreeNAS for the latest Nextcloud 18 release, including Caddy 1.0, MariaDB 10.3/PostgreSQL 10, and Let's Encrypt This script will create an iocage jail on FreeNAS 11.2-U7 or 11.3 with the latest release of Nextcloud 18, along with its dependencies. It will obtain a trusted certificate from Let's Encrypt for the system, install it, and configure it to renew automatically. It will create the Nextcloud database and generate a strong root password and user password for the database system. It will configure the jail to store the database and Nextcloud user data outside the jail, so it will not be lost in the event you need to rebuild the jail.","title":"freenas-iocage-nextcloud"},{"location":"blueprints/nextcloud/#status","text":"This script will work with FreeNAS 11.3, and it should also work with 11.2-U7. Due to the EOL status of FreeBSD 11.2, it is unlikely to work reliably with earlier releases of FreeNAS.","title":"Status"},{"location":"blueprints/nextcloud/#usage","text":"","title":"Usage"},{"location":"blueprints/nextcloud/#prerequisites-lets-encrypt","text":"This script works best when your installation is able to obtain a certificate from Let's Encrypt . When you use it this way, Caddy is able to handle all of the TLS-related configuration for you, obtain and renew certificates automatically, etc. In order for this to happen, you must meet the two requirements below: First, you must own or control a real Internet domain name. This script obtains a TLS encryption certificate from Let's Encrypt, who will only issue for public domain names. Thus, domains like cloud.local , mycloud.lan , or nextcloud.home won't work. Domains can be very inexpensive, and in some cases, they can be free. Freenom , for example, provides domains for free if you jump through the right hoops. EasyDNS is a fine domain registrar for paid domains, costing roughly US$15 per year (which varies slightly with the top-level domain). Second, one of these two conditions must be met in order for Let's Encrypt to validate your control over the domain name: You must be able and willing to open ports 80 and 443 from the entire Internet to the jail, and leave them open. If this applies, do it before running this script. DNS hosting for the domain name needs to be with a provider that Caddy supports, to automatically update the DNS records needed to prove your control over the domain. See the Caddy documentation under the heading of \"DNS Providers\" for the supported providers, and what information you'll need in order to proceed. Cloudflare provides DNS hosting at no cost, and it's well-supported by Caddy. Cloudflare doesn't directly provide Dynamic DNS service, but DNS-O-Matic is a Dynamic DNS provider that will interface with many DNS hosts including Cloudflare, and is also free of charge. So, even if you have a dynamic IP address (as most residential Internet users do), you don't have your own domain, and you aren't willing to pay for a domain or any other relevant service, and you aren't willing to open any ports from the Internet to your system, you can still get a trusted certificate from Let's Encrypt by following these steps: Register a free domain with Freenom. Be sure to keep up with the renewal requirements. Sign up for a free account with Cloudflare, and activate it for free DNS service only on your domain. Tell Freenom to use Cloudflare for DNS for your domain. Sign up for a free account with DNS-O-Matic, and configure it to update your Cloudflare DNS. Set up FreeNAS (see this thread ), your router, or whatever you prefer to update DNS-O-Matic as your IP address changes. Set up this script to do DNS validation, tell it to use the cloudflare plugin, and give it your email address and Global API key. If you aren't able or willing to obtain a certificate from Let's Encrypt, this script also supports configuring Caddy with a self-signed certificate, or with no certificate (and thus no HTTPS) at all.","title":"Prerequisites (Let's Encrypt)"},{"location":"blueprints/nextcloud/#prerequisites-other","text":"Although not required, it's recommended to create two datasets on your main storage pool: one named files , which will store the Nextcloud user data; and one called db , which will store the SQL database. For optimal performance, set the record size of the db dataset to 16 KB (under Advanced Settings in the FreeNAS web GUI). It's also recommended to cache only metadata on the db dataset; you can do this by running zfs set primarycache=metadata poolname/db .","title":"Prerequisites (Other)"},{"location":"blueprints/nextcloud/#installation","text":"Download the repository to a convenient directory on your FreeNAS system by running git clone https://github.com/danb35/freenas-iocage-nextcloud . Then change into the new directory and create a file called nextcloud-config . It should look like this: JAIL_IP=\"192.168.1.199\" DEFAULT_GW_IP=\"192.168.1.1\" POOL_PATH=\"/mnt/tank\" TIME_ZONE=\"America/New_York\" HOST_NAME=\"YOUR_FQDN\" STANDALONE_CERT=1 CERT_EMAIL=\"me@example.com\" Many of the options are self-explanatory, and all should be adjusted to suit your needs, but only a few are mandatory. The mandatory options are: JAIL_IP is the IP address for your jail DEFAULT_GW_IP is the address for your default gateway POOL_PATH is the path for your data pool. TIME_ZONE is the time zone of your location, in PHP notation--see the PHP manual for a list of all valid time zones. HOST_NAME is the fully-qualified domain name you want to assign to your installation. You must own (or at least control) this domain, because Let's Encrypt will test that control. DNS_CERT, STANDALONE_CERT, SELFSIGNED_CERT, and NO_CERT determine which method will be used to generate a TLS certificate (or, in the case of NO_CERT, indicate that you don't want to use SSL at all). DNS_CERT and STANDALONE_CERT indicate use of DNS or HTTP validation for Let's Encrypt, respectively. One and only one of these must be set to 1. CERT_EMAIL is the email address Let's Encrypt will use to notify you of certificate expiration. This is mandatory regardless of whether you're using Let's Encrypt (Caddy won't start without it), but it's only used with Let's Encrypt. If you are not using one of the Let's Encrypt certificate options, you can set this to a dummy address as above. If you are using Let's Encrypt, though, it should be set to a valid address for the system admin. DNS_PLUGIN: If DNS_CERT is set, DNS_PLUGIN must contain the name of the DNS validation plugin you'll use with Caddy to validate domain control. See the Caddy documentation under the heading of \"DNS Providers\" for the available plugins, but omit the leading \"tls.dns.\". For example, to use Cloudflare, set DNS_PLUGIN=\"cloudflare\" . DNS_ENV: If DNS_CERT is set, DNS_ENV must contain the authentication credentials for your DNS provider. See the Caddy documentation under the heading of \"DNS Providers\" for further details. For Cloudflare, you'd set DNS_ENV=\"CLOUDFLARE_EMAIL=foo@bar.baz CLOUDFLARE_API_KEY=blah\" , using your the email address of your Cloudflare account and your Global API key--the newer API tokens aren't currently supported. In addition, there are some other options which have sensible defaults, but can be adjusted if needed. These are: JAIL_NAME: The name of the jail, defaults to \"nextcloud\" DB_PATH, FILES_PATH, and PORTS_PATH: These are the paths to your database files, your data files, and the FreeBSD Ports collection. They default to $POOL_PATH/db, $POOL_PATH/files, and $POOL_PATH/portsnap, respectively. DATABASE: Which database management system to use. Default is \"mariadb\", but can be set to \"pgsql\" if you prefer to use PostgreSQL. INTERFACE: The network interface to use for the jail. Defaults to vnet0 . VNET: Whether to use the iocage virtual network stack. Defaults to on . If you're going to open ports 80 and 443 from the outside world to your jail, do so before running the script, and set STANDALONE_CERT to 1. If not, but you use a DNS provider that's supported by Caddy, set DNS_CERT to 1. If neither of these is true, you won't be able to use this script without modification. It's also helpful if HOST_NAME resolves to your jail from inside your network. You'll probably need to configure this on your router. If it doesn't, you'll still be able to reach your Nextcloud installation via the jail's IP address, but you'll get certificate errors that way.","title":"Installation"},{"location":"blueprints/nextcloud/#execution","text":"Once you've downloaded the script and prepared the configuration file, run this script ( ./nextcloud-jail.sh ). The script will run for several minutes. When it finishes, your jail will be created, Nextcloud will be installed and configured, and you'll be shown the randomly-generated password for the default user (\"admin\"). You can then log in and create users, add data, and generally do whatever else you like.","title":"Execution"},{"location":"blueprints/nextcloud/#obtaining-a-trusted-lets-encrypt-cert","text":"This configuration generated by this script will obtain certs from a non-trusted certificate authority by default. This is to prevent you from exhausting the Let's Encrypt rate limits while you're testing things out. Once you're sure things are working, you'll want to get a trusted cert instead. To do this, you can use a simple script that's included. As long as you haven't changed the default jail name, you can do this by running iocage exec nextcloud /root/remove-staging.sh .","title":"Obtaining a trusted Let's Encrypt cert"},{"location":"blueprints/nextcloud/#to-do","text":"I'd appreciate any suggestions (or, better yet, pull requests) to improve the various config files I'm using. Most of them are adapted from the default configuration files that ship with the software in question, and have only been lightly edited to work in this application. But if there are changes to settings or organization that could improve performance, reliability, or security, I'd like to hear about them.","title":"To Do"},{"location":"blueprints/organizr/","text":"Organizr \u00b6 Original README from the Organizr github repo: \u00b6 https://github.com/causefx/Organizr Do you have quite a bit of services running on your computer or server? Do you have a lot of bookmarks or have to memorize a bunch of ip's and ports? Well, Organizr is here to help with that. Organizr allows you to setup \"Tabs\" that will be loaded all in one webpage. You can then work on your server with ease. Want to give users access to some Tabs? No problem, just enable user support and have them make an account. Want guests to be able to visit too? Enable Guest support for those tabs. PHP 7.1.3+ Official Site - Will be refreshed soon! Official Discord See Wiki - Will be updated soon! Docker Login with Plex/Emby/LDAP or sFTP credentials Custom tabs for your services Fullscreen Support Pin/Unpin sidebar Mobile support Set default page on launch Upload new icons with ease Enable or disable iFrame for your tabs User management support: Create, delete and promote users from the user management console Unlimited User Groups Theme-able Personalise any theme: Customise the look and feel of Organizr with access to the colour palette Organizr login log viewer Fail2ban support (see wiki) Nginx Auth_Request support Protect new user account creation with registration password 'Forgot Password' support [receive an email with your new password, prerequisites: mail server setup] Multiple login support Keyboard shortcut support (Check help tab in settings) Gravatar Support Customise the top bar by adding your own site logo or site name Additional language support Quick access tabs [access your tabs quickly e.g. www.example.com/#Sonarr] Many more... Usage \u00b6 docker create \\ --name=organizr \\ -v <path to data>:/config \\ -e PGID=<gid> -e PUID=<uid> \\ -p 80:80 \\ organizrtools/organizr-v2 Parameters \u00b6 The parameters are split into two halves, separated by a colon, the left hand side representing the host and the right the container side. For example with a port -p external:internal - what this shows is the port mapping from internal to external of the container. So -p 8080:80 would expose port 80 from inside the container to be accessible from the host's IP on port 8080 and http://192.168.x.x:8080 would show you what's running INSIDE the container on port 80. -p 80 - The port(s) -v /config - Mapping the config files for Organizr -e PGID Used for GroupID - see below for explanation -e PUID Used for UserID - see below for explanation Info \u00b6 Shell access whilst the container is running: docker exec -it organizr /bin/bash To monitor the logs of the container in realtime: docker logs -f organizr Container version number: docker inspect -f '{{ index .Config.Labels \"build_version\" }}' organizr Image version number: docker inspect -f '{{ index .Config.Labels \"build_version\" }}' organizrtools/docker-organizr-v2 BrowserStack for allowing us to use their platform for testing This project is supported by:","title":"Organizr"},{"location":"blueprints/organizr/#organizr","text":"","title":"Organizr"},{"location":"blueprints/organizr/#original-readme-from-the-organizr-github-repo","text":"https://github.com/causefx/Organizr Do you have quite a bit of services running on your computer or server? Do you have a lot of bookmarks or have to memorize a bunch of ip's and ports? Well, Organizr is here to help with that. Organizr allows you to setup \"Tabs\" that will be loaded all in one webpage. You can then work on your server with ease. Want to give users access to some Tabs? No problem, just enable user support and have them make an account. Want guests to be able to visit too? Enable Guest support for those tabs. PHP 7.1.3+ Official Site - Will be refreshed soon! Official Discord See Wiki - Will be updated soon! Docker Login with Plex/Emby/LDAP or sFTP credentials Custom tabs for your services Fullscreen Support Pin/Unpin sidebar Mobile support Set default page on launch Upload new icons with ease Enable or disable iFrame for your tabs User management support: Create, delete and promote users from the user management console Unlimited User Groups Theme-able Personalise any theme: Customise the look and feel of Organizr with access to the colour palette Organizr login log viewer Fail2ban support (see wiki) Nginx Auth_Request support Protect new user account creation with registration password 'Forgot Password' support [receive an email with your new password, prerequisites: mail server setup] Multiple login support Keyboard shortcut support (Check help tab in settings) Gravatar Support Customise the top bar by adding your own site logo or site name Additional language support Quick access tabs [access your tabs quickly e.g. www.example.com/#Sonarr] Many more...","title":"Original README from the Organizr github repo:"},{"location":"blueprints/organizr/#usage","text":"docker create \\ --name=organizr \\ -v <path to data>:/config \\ -e PGID=<gid> -e PUID=<uid> \\ -p 80:80 \\ organizrtools/organizr-v2","title":"Usage"},{"location":"blueprints/organizr/#parameters","text":"The parameters are split into two halves, separated by a colon, the left hand side representing the host and the right the container side. For example with a port -p external:internal - what this shows is the port mapping from internal to external of the container. So -p 8080:80 would expose port 80 from inside the container to be accessible from the host's IP on port 8080 and http://192.168.x.x:8080 would show you what's running INSIDE the container on port 80. -p 80 - The port(s) -v /config - Mapping the config files for Organizr -e PGID Used for GroupID - see below for explanation -e PUID Used for UserID - see below for explanation","title":"Parameters"},{"location":"blueprints/organizr/#info","text":"Shell access whilst the container is running: docker exec -it organizr /bin/bash To monitor the logs of the container in realtime: docker logs -f organizr Container version number: docker inspect -f '{{ index .Config.Labels \"build_version\" }}' organizr Image version number: docker inspect -f '{{ index .Config.Labels \"build_version\" }}' organizrtools/docker-organizr-v2 BrowserStack for allowing us to use their platform for testing This project is supported by:","title":"Info"},{"location":"blueprints/plex/","text":"Plex \u00b6 Config Parameters: \u00b6 beta: set to true if you want to run the plex beta (previously known as \"plexpass\"). Please note: This is not required for plexpass features For more information about plex, please see the Plex website: Config Parameters: \u00b6 ramdisk: Specify the size parameter to create a transcoding ramdisk under /tmp_transcode. Requires manual setting it un plex to be used for transcoding. (optional) Original plex install script guide \u00b6 https://www.ixsystems.com/community/resources/fn11-3-iocage-jails-plex-tautulli-sonarr-radarr-lidarr-jackett-transmission-organizr.58/","title":"Plex"},{"location":"blueprints/plex/#plex","text":"","title":"Plex"},{"location":"blueprints/plex/#config-parameters","text":"beta: set to true if you want to run the plex beta (previously known as \"plexpass\"). Please note: This is not required for plexpass features For more information about plex, please see the Plex website:","title":"Config Parameters:"},{"location":"blueprints/plex/#config-parameters_1","text":"ramdisk: Specify the size parameter to create a transcoding ramdisk under /tmp_transcode. Requires manual setting it un plex to be used for transcoding. (optional)","title":"Config Parameters:"},{"location":"blueprints/plex/#original-plex-install-script-guide","text":"https://www.ixsystems.com/community/resources/fn11-3-iocage-jails-plex-tautulli-sonarr-radarr-lidarr-jackett-transmission-organizr.58/","title":"Original plex install script guide"},{"location":"blueprints/radarr/","text":"Radarr \u00b6 Original README from the radarr github: \u00b6 https://github.com/Radarr/Radarr Radarr \u00b6 New UI Development: For an overview of the new UI development see DEVELOPMENT.md . Radarr is an independent fork of Sonarr reworked for automatically downloading movies via Usenet and BitTorrent. The project was inspired by other Usenet/BitTorrent movie downloaders such as CouchPotato. See the Roadmap blogpost for an overview of planned features. Getting Started \u00b6 Install Radarr for your desired OS or use Docker For Linux users , run radarr and optionally have Radarr start automatically Connect to the UI through http://localhost:7878 or http://your-ip:7878 in your web browser See the Setup Guide for further configuration Downloads \u00b6 Release Type Branch: develop (stable) Branch: nightly (semi-unstable) Branch: aphrodite (very-unstable) Binary Releases Docker Docker Support \u00b6 Status \u00b6 Service Master Develop AppVeyor Travis Site and API Status \u00b6 API Updates Sites Radarr is currently undergoing rapid development and pull requests are actively added into the repository. Features \u00b6 Current Features \u00b6 Adding new movies with lots of information, such as trailers, ratings, etc. Support for major platforms: Windows, Linux, macOS, Raspberry Pi, etc. Can watch for better quality of the movies you have and do an automatic upgrade. eg. from DVD to Blu-Ray Automatic failed download handling will try another release if one fails Manual search so you can pick any release or to see why a release was not downloaded automatically Full integration with SABnzbd and NZBGet Automatically searching for releases as well as RSS Sync Automatically importing downloaded movies Recognizing Special Editions, Director's Cut, etc. Identifying releases with hardcoded subs All indexers supported by Sonarr also supported New PassThePopcorn Indexer QBittorrent, Deluge, rTorrent, Transmission and uTorrent download client (Other clients are coming) New TorrentPotato Indexer Torznab Indexer now supports Movies (Works well with Jackett ) Scanning PreDB to know when a new release is available Importing movies from various online sources, such as IMDb Watchlists (A complete list can be found here ) Full integration with Kodi, Plex (notification, library update) And a beautiful UI Importing Metadata such as trailers or subtitles Adding metadata such as posters and information for Kodi and others to use Advanced customization for profiles, such that Radarr will always download the copy you want Planned Features \u00b6 See the Roadmap blogpost for an overview of planned features. Feature Requests \u00b6 Configuring the Development Environment \u00b6 Requirements \u00b6 Visual Studio Community 2019 or Rider Git Node.js Yarn Setup \u00b6 Make sure all the required software mentioned above are installed Clone the repository into your development machine ( info ) Grab the submodules git submodule init && git submodule update Install the required Node Packages yarn install Start gulp to monitor your dev environment for any changes that need post processing using yarn start command. Notice Gulp must be running at all times while you are working with Radarr client source files. Build \u00b6 To build run sh build.sh Note: Windows users must have bash available to do this. If you installed git, you should have a git bash utility that works. Development \u00b6 Open Radarr.sln in Visual Studio 2017 or run the build.sh script, if Mono is installed. Alternatively you can use Jetbrains Rider, since it works on all Platforms. Make sure NzbDrone.Console is set as the startup project Run build.sh before running Supporters \u00b6 This project would not be possible without the support by these amazing folks. Become a sponsor or backer to help us out! Sponsors \u00b6 Flexible Sponsors \u00b6 Backers \u00b6 JetBrains \u00b6 Thank you to JetBrains for providing us with free licenses to their great tools ReSharper WebStorm Rider dotTrace License \u00b6 GNU GPL v3 Copyright 2010-2019","title":"Radarr"},{"location":"blueprints/radarr/#radarr","text":"","title":"Radarr"},{"location":"blueprints/radarr/#original-readme-from-the-radarr-github","text":"https://github.com/Radarr/Radarr","title":"Original README from the radarr github:"},{"location":"blueprints/radarr/#radarr_1","text":"New UI Development: For an overview of the new UI development see DEVELOPMENT.md . Radarr is an independent fork of Sonarr reworked for automatically downloading movies via Usenet and BitTorrent. The project was inspired by other Usenet/BitTorrent movie downloaders such as CouchPotato. See the Roadmap blogpost for an overview of planned features.","title":"Radarr"},{"location":"blueprints/radarr/#getting-started","text":"Install Radarr for your desired OS or use Docker For Linux users , run radarr and optionally have Radarr start automatically Connect to the UI through http://localhost:7878 or http://your-ip:7878 in your web browser See the Setup Guide for further configuration","title":"Getting Started"},{"location":"blueprints/radarr/#downloads","text":"Release Type Branch: develop (stable) Branch: nightly (semi-unstable) Branch: aphrodite (very-unstable) Binary Releases Docker Docker","title":"Downloads"},{"location":"blueprints/radarr/#support","text":"","title":"Support"},{"location":"blueprints/radarr/#status","text":"Service Master Develop AppVeyor Travis","title":"Status"},{"location":"blueprints/radarr/#site-and-api-status","text":"API Updates Sites Radarr is currently undergoing rapid development and pull requests are actively added into the repository.","title":"Site and API Status"},{"location":"blueprints/radarr/#features","text":"","title":"Features"},{"location":"blueprints/radarr/#current-features","text":"Adding new movies with lots of information, such as trailers, ratings, etc. Support for major platforms: Windows, Linux, macOS, Raspberry Pi, etc. Can watch for better quality of the movies you have and do an automatic upgrade. eg. from DVD to Blu-Ray Automatic failed download handling will try another release if one fails Manual search so you can pick any release or to see why a release was not downloaded automatically Full integration with SABnzbd and NZBGet Automatically searching for releases as well as RSS Sync Automatically importing downloaded movies Recognizing Special Editions, Director's Cut, etc. Identifying releases with hardcoded subs All indexers supported by Sonarr also supported New PassThePopcorn Indexer QBittorrent, Deluge, rTorrent, Transmission and uTorrent download client (Other clients are coming) New TorrentPotato Indexer Torznab Indexer now supports Movies (Works well with Jackett ) Scanning PreDB to know when a new release is available Importing movies from various online sources, such as IMDb Watchlists (A complete list can be found here ) Full integration with Kodi, Plex (notification, library update) And a beautiful UI Importing Metadata such as trailers or subtitles Adding metadata such as posters and information for Kodi and others to use Advanced customization for profiles, such that Radarr will always download the copy you want","title":"Current Features"},{"location":"blueprints/radarr/#planned-features","text":"See the Roadmap blogpost for an overview of planned features.","title":"Planned Features"},{"location":"blueprints/radarr/#feature-requests","text":"","title":"Feature Requests"},{"location":"blueprints/radarr/#configuring-the-development-environment","text":"","title":"Configuring the Development Environment"},{"location":"blueprints/radarr/#requirements","text":"Visual Studio Community 2019 or Rider Git Node.js Yarn","title":"Requirements"},{"location":"blueprints/radarr/#setup","text":"Make sure all the required software mentioned above are installed Clone the repository into your development machine ( info ) Grab the submodules git submodule init && git submodule update Install the required Node Packages yarn install Start gulp to monitor your dev environment for any changes that need post processing using yarn start command. Notice Gulp must be running at all times while you are working with Radarr client source files.","title":"Setup"},{"location":"blueprints/radarr/#build","text":"To build run sh build.sh Note: Windows users must have bash available to do this. If you installed git, you should have a git bash utility that works.","title":"Build"},{"location":"blueprints/radarr/#development","text":"Open Radarr.sln in Visual Studio 2017 or run the build.sh script, if Mono is installed. Alternatively you can use Jetbrains Rider, since it works on all Platforms. Make sure NzbDrone.Console is set as the startup project Run build.sh before running","title":"Development"},{"location":"blueprints/radarr/#supporters","text":"This project would not be possible without the support by these amazing folks. Become a sponsor or backer to help us out!","title":"Supporters"},{"location":"blueprints/radarr/#sponsors","text":"","title":"Sponsors"},{"location":"blueprints/radarr/#flexible-sponsors","text":"","title":"Flexible Sponsors"},{"location":"blueprints/radarr/#backers","text":"","title":"Backers"},{"location":"blueprints/radarr/#jetbrains","text":"Thank you to JetBrains for providing us with free licenses to their great tools ReSharper WebStorm Rider dotTrace","title":"JetBrains"},{"location":"blueprints/radarr/#license","text":"GNU GPL v3 Copyright 2010-2019","title":"License"},{"location":"blueprints/sonarr/","text":"Sonarr \u00b6 Original README from the sonarr github: \u00b6 https://github.com/Sonarr/Sonarr Sonarr \u00b6 Sonarr is a PVR for Usenet and BitTorrent users. It can monitor multiple RSS feeds for new episodes of your favorite shows and will grab, sort and rename them. It can also be configured to automatically upgrade the quality of files already downloaded when a better quality format becomes available. Major Features Include: \u00b6 Support for major platforms: Windows, Linux, macOS, Raspberry Pi, etc. Automatically detects new episodes Can scan your existing library and download any missing episodes Can watch for better quality of the episodes you already have and do an automatic upgrade. eg. from DVD to Blu-Ray Automatic failed download handling will try another release if one fails Manual search so you can pick any release or to see why a release was not downloaded automatically Fully configurable episode renaming Full integration with SABnzbd and NZBGet Full integration with Kodi, Plex (notification, library update, metadata) Full support for specials and multi-episode releases And a beautiful UI Configuring Development Environment: \u00b6 Requirements \u00b6 Visual Studio 2017 Git NodeJS Yarn Setup \u00b6 Make sure all the required software mentioned above are installed Clone the repository into your development machine. info Grab the submodules git submodule init && git submodule update Install the required Node Packages yarn Backend Development \u00b6 Run yarn build to build the UI Open Sonarr.sln in Visual Studio Make sure NzbDrone.Console is set as the startup project Build NzbDrone.Windows and NzbDrone.Mono projects Build Solution UI Development \u00b6 Run yarn watch to build UI and rebuild automatically when changes are detected Run Sonarr.Console.exe (or debug in Visual Studio) License \u00b6 GNU GPL v3 Copyright 2010-2019 Sponsors \u00b6 JetBrains for providing us with free licenses to their great tools ReSharper TeamCity","title":"Sonarr"},{"location":"blueprints/sonarr/#sonarr","text":"","title":"Sonarr"},{"location":"blueprints/sonarr/#original-readme-from-the-sonarr-github","text":"https://github.com/Sonarr/Sonarr","title":"Original README from the sonarr github:"},{"location":"blueprints/sonarr/#sonarr_1","text":"Sonarr is a PVR for Usenet and BitTorrent users. It can monitor multiple RSS feeds for new episodes of your favorite shows and will grab, sort and rename them. It can also be configured to automatically upgrade the quality of files already downloaded when a better quality format becomes available.","title":"Sonarr"},{"location":"blueprints/sonarr/#major-features-include","text":"Support for major platforms: Windows, Linux, macOS, Raspberry Pi, etc. Automatically detects new episodes Can scan your existing library and download any missing episodes Can watch for better quality of the episodes you already have and do an automatic upgrade. eg. from DVD to Blu-Ray Automatic failed download handling will try another release if one fails Manual search so you can pick any release or to see why a release was not downloaded automatically Fully configurable episode renaming Full integration with SABnzbd and NZBGet Full integration with Kodi, Plex (notification, library update, metadata) Full support for specials and multi-episode releases And a beautiful UI","title":"Major Features Include:"},{"location":"blueprints/sonarr/#configuring-development-environment","text":"","title":"Configuring Development Environment:"},{"location":"blueprints/sonarr/#requirements","text":"Visual Studio 2017 Git NodeJS Yarn","title":"Requirements"},{"location":"blueprints/sonarr/#setup","text":"Make sure all the required software mentioned above are installed Clone the repository into your development machine. info Grab the submodules git submodule init && git submodule update Install the required Node Packages yarn","title":"Setup"},{"location":"blueprints/sonarr/#backend-development","text":"Run yarn build to build the UI Open Sonarr.sln in Visual Studio Make sure NzbDrone.Console is set as the startup project Build NzbDrone.Windows and NzbDrone.Mono projects Build Solution","title":"Backend Development"},{"location":"blueprints/sonarr/#ui-development","text":"Run yarn watch to build UI and rebuild automatically when changes are detected Run Sonarr.Console.exe (or debug in Visual Studio)","title":"UI Development"},{"location":"blueprints/sonarr/#license","text":"GNU GPL v3 Copyright 2010-2019","title":"License"},{"location":"blueprints/sonarr/#sponsors","text":"JetBrains for providing us with free licenses to their great tools ReSharper TeamCity","title":"Sponsors"},{"location":"blueprints/tautulli/","text":"Tautulli \u00b6 Original README from the tautulli github: \u00b6 https://github.com/Tautulli/Tautulli Tautulli \u00b6 A python based web application for monitoring, analytics and notifications for Plex Media Server . This project is based on code from Headphones and PlexWatchWeb . Features \u00b6 Responsive web design viewable on desktop, tablet and mobile web browsers. Themed to complement Plex/Web. Easy configuration setup (no separate web server required). Monitor current Plex Media Server activity. Fully customizable notifications for stream activity and recently added media. Top statistics on home page with configurable duration and measurement metric. Global watching history with search/filtering & dynamic column sorting. Full user list with general information and comparison stats. Individual user information including devices IP addresses. Complete library statistics and media file information. Rich analytics presented using Highcharts graphing. Beautiful content information pages. Full sync list data on all users syncing items from your library. And many more!! Preview \u00b6 Full preview gallery available on our website Installation & Support \u00b6 Status Branch: master Branch: beta Branch: nightly Release Docker Read the Installation Guides for instructions to install Tautulli. The Frequently Asked Questions in the wiki can help you with common problems. Support is available on Discord , Reddit , or the Plex Forums . Issues & Feature Requests \u00b6 Please see the Issues Repository . License \u00b6 This is free software under the GPL v3 open source license. Feel free to do with it what you wish, but any modification must be open sourced. A copy of the license is included. This software includes Highsoft software libraries which you may freely distribute for non-commercial use. Commerical users must licence this software, for more information visit https://shop.highsoft.com/faq/non-commercial#non-commercial-redistribution.","title":"Tautulli"},{"location":"blueprints/tautulli/#tautulli","text":"","title":"Tautulli"},{"location":"blueprints/tautulli/#original-readme-from-the-tautulli-github","text":"https://github.com/Tautulli/Tautulli","title":"Original README from the tautulli github:"},{"location":"blueprints/tautulli/#tautulli_1","text":"A python based web application for monitoring, analytics and notifications for Plex Media Server . This project is based on code from Headphones and PlexWatchWeb .","title":"Tautulli"},{"location":"blueprints/tautulli/#features","text":"Responsive web design viewable on desktop, tablet and mobile web browsers. Themed to complement Plex/Web. Easy configuration setup (no separate web server required). Monitor current Plex Media Server activity. Fully customizable notifications for stream activity and recently added media. Top statistics on home page with configurable duration and measurement metric. Global watching history with search/filtering & dynamic column sorting. Full user list with general information and comparison stats. Individual user information including devices IP addresses. Complete library statistics and media file information. Rich analytics presented using Highcharts graphing. Beautiful content information pages. Full sync list data on all users syncing items from your library. And many more!!","title":"Features"},{"location":"blueprints/tautulli/#preview","text":"Full preview gallery available on our website","title":"Preview"},{"location":"blueprints/tautulli/#installation-support","text":"Status Branch: master Branch: beta Branch: nightly Release Docker Read the Installation Guides for instructions to install Tautulli. The Frequently Asked Questions in the wiki can help you with common problems. Support is available on Discord , Reddit , or the Plex Forums .","title":"Installation &amp; Support"},{"location":"blueprints/tautulli/#issues-feature-requests","text":"Please see the Issues Repository .","title":"Issues &amp; Feature Requests"},{"location":"blueprints/tautulli/#license","text":"This is free software under the GPL v3 open source license. Feel free to do with it what you wish, but any modification must be open sourced. A copy of the license is included. This software includes Highsoft software libraries which you may freely distribute for non-commercial use. Commerical users must licence this software, for more information visit https://shop.highsoft.com/faq/non-commercial#non-commercial-redistribution.","title":"License"},{"location":"blueprints/transmission/","text":"Transmission \u00b6 Original README from the transmission github: \u00b6 https://github.com/transmission/transmission Transmission \u00b6 About \u00b6 Transmission is a fast, easy, and free BitTorrent client. It comes in several flavors: * A native Mac OS X GUI application * GTK+ and Qt GUI applications for Linux, BSD, etc. * A headless daemon for servers and routers * A web UI for remote controlling any of the above Visit https://transmissionbt.com/ for more information. Command line interface notes \u00b6 Transmission is fully supported in transmission-remote, the preferred cli client. Three standalone tools to examine, create, and edit .torrent files exist: transmission-show, transmission-create, and transmission-edit, respectively. Prior to development of transmission-remote, the standalone client transmission-cli was created. Limited to a single torrent at a time, transmission-cli is deprecated and exists primarily to support older hardware dependent upon it. In almost all instances, transmission-remote should be used instead. Different distributions may choose to package any or all of these tools in one or more separate packages. Building \u00b6 Transmission has an Xcode project file (Transmission.xcodeproj) for building in Xcode. For a more detailed description, and dependencies, visit: https://github.com/transmission/transmission/wiki Building a Transmission release from the command line \u00b6 $ tar xf transmission-2.92.tar.xz $ cd transmission-2.92 $ mkdir build $ cd build $ cmake .. $ make $ sudo make install Building Transmission from the nightly builds \u00b6 Download a tarball from https://build.transmissionbt.com/job/trunk-linux/ and follow the steps from the previous section. If you're new to building programs from source code, this is typically easier than building from Git. Building Transmission from Git (first time) \u00b6 $ git clone https://github.com/transmission/transmission Transmission $ cd Transmission $ git submodule update --init $ mkdir build $ cd build $ cmake .. $ make $ sudo make install Building Transmission from Git (updating) \u00b6 $ cd Transmission/build $ make clean $ git pull --rebase --prune $ git submodule update $ cmake .. $ make $ sudo make install Contributing \u00b6 Code Style \u00b6 You would want to setup your editor to make use of uncrustify.cfg and .jsbeautifyrc configuration files located in the root of this repository. If for some reason you are unwilling or unable to do so, there is a shell script which you could run either directly or via docker-compose: $ ./code_style.sh or $ docker-compose build --pull $ docker-compose run --rm code_style","title":"Transmission"},{"location":"blueprints/transmission/#transmission","text":"","title":"Transmission"},{"location":"blueprints/transmission/#original-readme-from-the-transmission-github","text":"https://github.com/transmission/transmission","title":"Original README from the transmission github:"},{"location":"blueprints/transmission/#transmission_1","text":"","title":"Transmission"},{"location":"blueprints/transmission/#about","text":"Transmission is a fast, easy, and free BitTorrent client. It comes in several flavors: * A native Mac OS X GUI application * GTK+ and Qt GUI applications for Linux, BSD, etc. * A headless daemon for servers and routers * A web UI for remote controlling any of the above Visit https://transmissionbt.com/ for more information.","title":"About"},{"location":"blueprints/transmission/#command-line-interface-notes","text":"Transmission is fully supported in transmission-remote, the preferred cli client. Three standalone tools to examine, create, and edit .torrent files exist: transmission-show, transmission-create, and transmission-edit, respectively. Prior to development of transmission-remote, the standalone client transmission-cli was created. Limited to a single torrent at a time, transmission-cli is deprecated and exists primarily to support older hardware dependent upon it. In almost all instances, transmission-remote should be used instead. Different distributions may choose to package any or all of these tools in one or more separate packages.","title":"Command line interface notes"},{"location":"blueprints/transmission/#building","text":"Transmission has an Xcode project file (Transmission.xcodeproj) for building in Xcode. For a more detailed description, and dependencies, visit: https://github.com/transmission/transmission/wiki","title":"Building"},{"location":"blueprints/transmission/#building-a-transmission-release-from-the-command-line","text":"$ tar xf transmission-2.92.tar.xz $ cd transmission-2.92 $ mkdir build $ cd build $ cmake .. $ make $ sudo make install","title":"Building a Transmission release from the command line"},{"location":"blueprints/transmission/#building-transmission-from-the-nightly-builds","text":"Download a tarball from https://build.transmissionbt.com/job/trunk-linux/ and follow the steps from the previous section. If you're new to building programs from source code, this is typically easier than building from Git.","title":"Building Transmission from the nightly builds"},{"location":"blueprints/transmission/#building-transmission-from-git-first-time","text":"$ git clone https://github.com/transmission/transmission Transmission $ cd Transmission $ git submodule update --init $ mkdir build $ cd build $ cmake .. $ make $ sudo make install","title":"Building Transmission from Git (first time)"},{"location":"blueprints/transmission/#building-transmission-from-git-updating","text":"$ cd Transmission/build $ make clean $ git pull --rebase --prune $ git submodule update $ cmake .. $ make $ sudo make install","title":"Building Transmission from Git (updating)"},{"location":"blueprints/transmission/#contributing","text":"","title":"Contributing"},{"location":"blueprints/transmission/#code-style","text":"You would want to setup your editor to make use of uncrustify.cfg and .jsbeautifyrc configuration files located in the root of this repository. If for some reason you are unwilling or unable to do so, there is a shell script which you could run either directly or via docker-compose: $ ./code_style.sh or $ docker-compose build --pull $ docker-compose run --rm code_style","title":"Code Style"},{"location":"blueprints/unifi/","text":"Unifi Controller \u00b6 Installation: \u00b6 This jail requires an existing InfluxDB jail. InfluxDB may be created using the same install command, as long as influxdb is listed first. Once the script runs, a user must be created in the Unifi Controller software for your Unifi-Poller user. To view the data from Unifi-Poller, Grafana is required. Add the unifi InfluxDB database as a data source in Grafana. Config Description \u00b6 unifi_poller: boolean, true if you want to also install unifi-poller db_jail: This is the name of your influxdb database jail, should be influxdb. unifi_db_name: The name of the database that will be created in influxdb for Unifi Poller. unifi_db_user & unifi_db_password: The created database's credentials for Unifi Poller. up_user & up_password: The Unifi-Poller user credentials. This user must be created in the Unifi Controller web gui after install matching these credentials. This is for the connection between Unifi Controller & Unifi Poller Unifi-Controller Post-Install \u00b6 After the script runs and the unifi jail is running, open the web gui of the unifi jail at port 8443 (i.e. https://192.168.2.250:8443). After completing the initial setup wizard, go to Admins --> Add New Admin. Select \"Manually set and share the password\", enter the username and password used for up_user & up_password. Uncheck 'Require the user to change their password'. Verify \"Role\" is set to 'Read Only'. Click Create. Unifi Controller documentation can be found at https://www.ui.com/download/unifi/default/default/unifi-controller-v5-user-guide \u00b6 Original README from the upstream Unifi-Poller Github. \u00b6 https://github.com/unifi-poller/unifi-poller Collect your UniFi controller data and report it to an InfluxDB instance, or export it for Prometheus collection. Twelve Grafana Dashboards included; with screenshots. Six for InfluxDB and six for Prometheus. Installation \u00b6 See the Wiki! We have a special place for Docker Users . I'm willing to help if you have troubles. Open an Issue and we'll figure out how to get things working for you. You can also get help in the #unifi-poller channel on the Ubiquiti Discord server . I've also provided a forum post you may use to get additional help. Description \u00b6 Ubiquiti makes networking devices like switches, gateways (routers) and wireless access points. They have a line of equipment named UniFi that uses a controller to keep stats and simplify network device configuration. This controller can be installed on Windows, macOS, FreeBSD, Linux or Docker. Ubiquiti also provides a dedicated hardware device called a CloudKey that runs the controller software. More recently they've developed the Dream Machine; it's still in beta / early access, but UniFi Poller can collect its data! UniFi Poller is a small Golang application that runs on Windows, macOS, FreeBSD, Linux or Docker. In Influx-mode it polls a UniFi controller every 30 seconds for measurements and exports the data to an Influx database. In Prometheus mode the poller opens a web port and accepts Prometheus polling. It converts the UniFi Controller API data into Prometheus exports on the fly. This application requires your controller to be running all the time. If you run a UniFi controller, there's no excuse not to install Influx or Prometheus , Grafana and this app. You'll have a plethora of data at your fingertips and the ability to craft custom graphs to slice the data any way you choose. Good luck! Backstory \u00b6 I found a simple piece of code on GitHub that sorta did what I needed; we all know that story. I wanted more data, so I added more data collection. I probably wouldn't have made it this far if Garrett hadn't written the original code I started with. Many props my man. The original code pulled only the client data. This app now pulls data for clients, access points, security gateways, dream machines and switches. I've been trying to get my UAP data into Grafana. Sure, google search that. You'll find this . What if you don't want to deal with SNMP? Well, here you go. I've replicated 400% of what you see on those SNMP-powered dashboards with this Go app running on the same mac as my UniFi controller. All without enabling SNMP nor trying to understand those OIDs. Mad props to waterside for making this dashboard; it gave me a fantastic start to making my own dashboards. Operation \u00b6 You can control this app with puppet, chef, saltstack, homebrew or a simple bash script if you needed to. Packages are available for macOS, Linux, FreeBSD and Docker. It works just fine on Windows too. Most people prefer Docker, and this app is right at home in that environment. What's it look like? \u00b6 There are 12 total dashboards available; the 6 InfluxDB dashboards are very similar to the 6 Prometheus dashboards. Below you'll find screenshots of the first four dashboards. Client Dashboard (InfluxDB) \u00b6 USG Dashboard (InfluxDB) \u00b6 UAP Dashboard (InfluxDB) \u00b6 USW / Switch Dashboard (InfluxDB) \u00b6 You can drill down into specific sites, switches, and ports. Compare ports in different sites side-by-side. So easy! This screenshot barely does it justice. Integrations \u00b6 The following fine folks are providing their services, completely free! These service integrations are used for things like storage, building, compiling, distribution and documentation support. This project succeeds because of them. Thank you! Copyright & License \u00b6 Copyright \u00a9 2018-2020 David Newhall II. See LICENSE for license information.","title":"Unifi Controller"},{"location":"blueprints/unifi/#unifi-controller","text":"","title":"Unifi Controller"},{"location":"blueprints/unifi/#installation","text":"This jail requires an existing InfluxDB jail. InfluxDB may be created using the same install command, as long as influxdb is listed first. Once the script runs, a user must be created in the Unifi Controller software for your Unifi-Poller user. To view the data from Unifi-Poller, Grafana is required. Add the unifi InfluxDB database as a data source in Grafana.","title":"Installation:"},{"location":"blueprints/unifi/#config-description","text":"unifi_poller: boolean, true if you want to also install unifi-poller db_jail: This is the name of your influxdb database jail, should be influxdb. unifi_db_name: The name of the database that will be created in influxdb for Unifi Poller. unifi_db_user & unifi_db_password: The created database's credentials for Unifi Poller. up_user & up_password: The Unifi-Poller user credentials. This user must be created in the Unifi Controller web gui after install matching these credentials. This is for the connection between Unifi Controller & Unifi Poller","title":"Config Description"},{"location":"blueprints/unifi/#unifi-controller-post-install","text":"After the script runs and the unifi jail is running, open the web gui of the unifi jail at port 8443 (i.e. https://192.168.2.250:8443). After completing the initial setup wizard, go to Admins --> Add New Admin. Select \"Manually set and share the password\", enter the username and password used for up_user & up_password. Uncheck 'Require the user to change their password'. Verify \"Role\" is set to 'Read Only'. Click Create.","title":"Unifi-Controller Post-Install"},{"location":"blueprints/unifi/#unifi-controller-documentation-can-be-found-at-httpswwwuicomdownloadunifidefaultdefaultunifi-controller-v5-user-guide","text":"","title":"Unifi Controller documentation can be found at https://www.ui.com/download/unifi/default/default/unifi-controller-v5-user-guide"},{"location":"blueprints/unifi/#original-readme-from-the-upstream-unifi-poller-github","text":"https://github.com/unifi-poller/unifi-poller Collect your UniFi controller data and report it to an InfluxDB instance, or export it for Prometheus collection. Twelve Grafana Dashboards included; with screenshots. Six for InfluxDB and six for Prometheus.","title":"Original README from the upstream Unifi-Poller Github."},{"location":"blueprints/unifi/#installation_1","text":"See the Wiki! We have a special place for Docker Users . I'm willing to help if you have troubles. Open an Issue and we'll figure out how to get things working for you. You can also get help in the #unifi-poller channel on the Ubiquiti Discord server . I've also provided a forum post you may use to get additional help.","title":"Installation"},{"location":"blueprints/unifi/#description","text":"Ubiquiti makes networking devices like switches, gateways (routers) and wireless access points. They have a line of equipment named UniFi that uses a controller to keep stats and simplify network device configuration. This controller can be installed on Windows, macOS, FreeBSD, Linux or Docker. Ubiquiti also provides a dedicated hardware device called a CloudKey that runs the controller software. More recently they've developed the Dream Machine; it's still in beta / early access, but UniFi Poller can collect its data! UniFi Poller is a small Golang application that runs on Windows, macOS, FreeBSD, Linux or Docker. In Influx-mode it polls a UniFi controller every 30 seconds for measurements and exports the data to an Influx database. In Prometheus mode the poller opens a web port and accepts Prometheus polling. It converts the UniFi Controller API data into Prometheus exports on the fly. This application requires your controller to be running all the time. If you run a UniFi controller, there's no excuse not to install Influx or Prometheus , Grafana and this app. You'll have a plethora of data at your fingertips and the ability to craft custom graphs to slice the data any way you choose. Good luck!","title":"Description"},{"location":"blueprints/unifi/#backstory","text":"I found a simple piece of code on GitHub that sorta did what I needed; we all know that story. I wanted more data, so I added more data collection. I probably wouldn't have made it this far if Garrett hadn't written the original code I started with. Many props my man. The original code pulled only the client data. This app now pulls data for clients, access points, security gateways, dream machines and switches. I've been trying to get my UAP data into Grafana. Sure, google search that. You'll find this . What if you don't want to deal with SNMP? Well, here you go. I've replicated 400% of what you see on those SNMP-powered dashboards with this Go app running on the same mac as my UniFi controller. All without enabling SNMP nor trying to understand those OIDs. Mad props to waterside for making this dashboard; it gave me a fantastic start to making my own dashboards.","title":"Backstory"},{"location":"blueprints/unifi/#operation","text":"You can control this app with puppet, chef, saltstack, homebrew or a simple bash script if you needed to. Packages are available for macOS, Linux, FreeBSD and Docker. It works just fine on Windows too. Most people prefer Docker, and this app is right at home in that environment.","title":"Operation"},{"location":"blueprints/unifi/#whats-it-look-like","text":"There are 12 total dashboards available; the 6 InfluxDB dashboards are very similar to the 6 Prometheus dashboards. Below you'll find screenshots of the first four dashboards.","title":"What's it look like?"},{"location":"blueprints/unifi/#client-dashboard-influxdb","text":"","title":"Client Dashboard (InfluxDB)"},{"location":"blueprints/unifi/#usg-dashboard-influxdb","text":"","title":"USG Dashboard (InfluxDB)"},{"location":"blueprints/unifi/#uap-dashboard-influxdb","text":"","title":"UAP Dashboard (InfluxDB)"},{"location":"blueprints/unifi/#usw-switch-dashboard-influxdb","text":"You can drill down into specific sites, switches, and ports. Compare ports in different sites side-by-side. So easy! This screenshot barely does it justice.","title":"USW / Switch Dashboard (InfluxDB)"},{"location":"blueprints/unifi/#integrations","text":"The following fine folks are providing their services, completely free! These service integrations are used for things like storage, building, compiling, distribution and documentation support. This project succeeds because of them. Thank you!","title":"Integrations"},{"location":"blueprints/unifi/#copyright-license","text":"Copyright \u00a9 2018-2020 David Newhall II. See LICENSE for license information.","title":"Copyright &amp; License"},{"location":"migration/v1.1.x%20to%20v1.2.x/","text":"v1.1.x to v1.2.x \u00b6 With v1.2 we made it possible to run multiple jails of the same type. This is done by seperating jails (your individual installs) from blueprints (our designs). Due to this change, the config file has been changed and thus you need to adapt your config file. Jails \u00b6 All your jails need to be indented by 2 spaces under a main group \"jails\" like this: jail: plex: blueprint: plex ip4_addr: 192.168.1.99/24 gateway: 192.168.1.1 beta: false Also note: Where previously we used plex: plex , we can now just use plex: Blueprints \u00b6 Every jail now requires a blueprint to be defined. for example: v1.1.x ` plex: plex ip4_addr: 192.168.1.99/24 gateway: 192.168.1.1 plexpass: false ` In v1.2.x becomes: jail: plex: blueprint: plex ip4_addr: 192.168.1.99/24 gateway: 192.168.1.1 beta: false Plex \u00b6 Due to community feedback in v1.2.x \"plexpass\" has been renamed to \"beta\". pkgs \u00b6 pkgs are removed from jail config in v1.2.x and are now part of the blueprint. This way we can keep them up-to-date for you. Documentation \u00b6 All jails now have wiki documentation pages and all basic jail values have been documented Other changes \u00b6 Some other minor changes in config file values have been done, please refer to the example document and compare your config to the example","title":"V1.1.x to v1.2.x"},{"location":"migration/v1.1.x%20to%20v1.2.x/#v11x-to-v12x","text":"With v1.2 we made it possible to run multiple jails of the same type. This is done by seperating jails (your individual installs) from blueprints (our designs). Due to this change, the config file has been changed and thus you need to adapt your config file.","title":"v1.1.x to v1.2.x"},{"location":"migration/v1.1.x%20to%20v1.2.x/#jails","text":"All your jails need to be indented by 2 spaces under a main group \"jails\" like this: jail: plex: blueprint: plex ip4_addr: 192.168.1.99/24 gateway: 192.168.1.1 beta: false Also note: Where previously we used plex: plex , we can now just use plex:","title":"Jails"},{"location":"migration/v1.1.x%20to%20v1.2.x/#blueprints","text":"Every jail now requires a blueprint to be defined. for example: v1.1.x ` plex: plex ip4_addr: 192.168.1.99/24 gateway: 192.168.1.1 plexpass: false ` In v1.2.x becomes: jail: plex: blueprint: plex ip4_addr: 192.168.1.99/24 gateway: 192.168.1.1 beta: false","title":"Blueprints"},{"location":"migration/v1.1.x%20to%20v1.2.x/#plex","text":"Due to community feedback in v1.2.x \"plexpass\" has been renamed to \"beta\".","title":"Plex"},{"location":"migration/v1.1.x%20to%20v1.2.x/#pkgs","text":"pkgs are removed from jail config in v1.2.x and are now part of the blueprint. This way we can keep them up-to-date for you.","title":"pkgs"},{"location":"migration/v1.1.x%20to%20v1.2.x/#documentation","text":"All jails now have wiki documentation pages and all basic jail values have been documented","title":"Documentation"},{"location":"migration/v1.1.x%20to%20v1.2.x/#other-changes","text":"Some other minor changes in config file values have been done, please refer to the example document and compare your config to the example","title":"Other changes"}]}